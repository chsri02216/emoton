{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bLx505BiP7huKcxYXdB6NJdOyBmmF8KB",
      "authorship_tag": "ABX9TyOTX4bC3tDhwNq407ddMBBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chsri02216/emoton/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BiTJ-8u5fUA5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "edfa57e8-ca94-46ba-e24b-6b287a129e31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 136\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyHpccqGPC4S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "#/content/drive/MyDrive/probe/fer2013.csv\n",
        "data = pd.read_csv(\"drive/MyDrive/probe/fer2013.csv\")\n",
        "\n",
        "#data = pd.read_csv(\"https://drive.google.com/file/d/1PpaJ8vAvw_6lK0Tu8_0xwMtyrmNIAodc/view?usp=sharing.csv\")\n",
        "data.head()\n",
        "\n",
        "\n",
        "labels = []\n",
        "usage = []\n",
        "\n",
        "for i in data[\"emotion\"]:\n",
        "    labels.append(i)\n",
        "    \n",
        "for i in data[\"Usage\"]:\n",
        "    usage.append(i)\n",
        "    \n",
        "print(set(labels))\n",
        "print(set(usage))\n",
        "\n",
        "\n",
        "\n",
        "count0 = 0\n",
        "count1 = 0\n",
        "count2 = 0\n",
        "count3 = 0\n",
        "count4 = 0\n",
        "count5 = 0\n",
        "count6 = 0\n",
        "\n",
        "for i, j, k in tqdm(zip(data[\"emotion\"], data[\"pixels\"], data[\"Usage\"])):\n",
        "    pixel = []\n",
        "    #print(pixel)\n",
        "    pixels = j.split(' ')\n",
        "    for m in pixels:\n",
        "        value = float(m)\n",
        "        pixel.append(value)\n",
        "    pixel = np.array(pixel)\n",
        "    image = pixel.reshape(48, 48)\n",
        "    \n",
        "    if k == \"Training\":\n",
        "        if not os.path.exists(\"drive/MyDrive/probe/train\"):\n",
        "            os.mkdir(\"drive/MyDrive/probe/train\")\n",
        "        if i == 0:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Angry\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Angry\")\n",
        "                path = \"drive/MyDrive/probe/train/Angry/\" + str(count0) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count0 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Angry/\" + str(count0) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count0 += 1\n",
        "        if i == 1:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Disgust\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Disgust\")\n",
        "                path = \"drive/MyDrive/probe/train/Disgust/\" + str(count1) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count1 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Disgust/\" + str(count1) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count1 += 1\n",
        "        if i == 2:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Fear\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Fear\")\n",
        "                path = \"drive/MyDrive/probe/train/Fear/\" + str(count2) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count2 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Fear/\" + str(count2) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count2 += 1\n",
        "        if i == 3:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Happy\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Happy\")\n",
        "                path = \"drive/MyDrive/probe/train/Happy/\" + str(count3) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count3 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Happy/\" + str(count3) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count3 += 1\n",
        "        if i == 4:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Sad\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Sad\")\n",
        "                path = \"drive/MyDrive/probe/train/Sad/\" + str(count4) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count4 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Sad/\" + str(count4) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count4 += 1\n",
        "        if i == 5:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Surprise\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Surprise\")\n",
        "                path = \"drive/MyDrive/probe/train/Surprise/\" + str(count5) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count5 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Surprise/\" + str(count5) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count5 += 1\n",
        "        if i == 6:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Neutral\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Neutral\")\n",
        "                path = \"drive/MyDrive/probe/train/Neutral/\" + str(count6) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count6 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Neutral/\" + str(count6) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count6 += 1\n",
        "    else:\n",
        "        if not os.path.exists(\"drive/MyDrive/probe/validation\"):\n",
        "            os.mkdir(\"drive/MyDrive/probe/validation\")\n",
        "        if i == 0:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Angry\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Angry\")\n",
        "                path = \"drive/MyDrive/probe/validation/Angry/\" + str(count0) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count0 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Angry/\" + str(count0) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count0 += 1\n",
        "        if i == 1:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Disgust\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Disgust\")\n",
        "                path = \"drive/MyDrive/probe/validation/Disgust/\" + str(count1) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count1 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Disgust/\" + str(count1) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count1 += 1\n",
        "        if i == 2:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Fear\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Fear\")\n",
        "                path = \"drive/MyDrive/probe/validation/Fear/\" + str(count2) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count2 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Fear/\" + str(count2) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count2 += 1\n",
        "        if i == 3:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Happy\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Happy\")\n",
        "                path = \"drive/MyDrive/probe/validation/Happy/\" + str(count3) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count3 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Happy/\" + str(count3) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count3 += 1\n",
        "        if i == 4:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Sad\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Sad\")\n",
        "                path = \"drive/MyDrive/probe/validation/Sad/\" + str(count4) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count4 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Sad/\" + str(count4) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count4 += 1\n",
        "        if i == 5:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Surprise\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Surprise\")\n",
        "                path = \"drive/MyDrive/probe/validation/Surprise/\" + str(count5) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count5 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Surprise/\" + str(count5) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count5 += 1\n",
        "        if i == 6:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Neutral\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Neutral\")\n",
        "                path = \"drive/MyDrive/probe/validation/Neutral/\" + str(count6) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count6 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Neutral/\" + str(count6) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count6 += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_directory = \"drive/MyDrive/probe/train/\"\n",
        "train_dir = os.listdir(train_directory)\n",
        "classes = []\n",
        "\n",
        "for folder in train_dir:\n",
        "    classes.append(folder)\n",
        "\n",
        "train_counts = []\n",
        "\n",
        "for folder in train_dir:\n",
        "    class_path = train_directory + folder + \"/\"\n",
        "    list_train = []\n",
        "    count = 0\n",
        "    for file in os.listdir(class_path):\n",
        "        count +=1\n",
        "    \n",
        "    train_counts.append(count)\n",
        "    \n",
        "train_counts\n",
        "\n",
        "\n",
        "# plt.bar(classes, train_counts, width=0.5)\n",
        "# plt.title(\"Bar Graph of Train Data\")\n",
        "# plt.xlabel(\"Classes\")\n",
        "# plt.ylabel(\"Counts\")\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(classes, train_counts)\n",
        "plt.plot(classes, train_counts, '-o')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F_9hoOFmPHq7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "117b4cc0-0ab2-4ac3-d747-eaea531d5a20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "image = cv2.imread('/content/drive/MyDrive/probe/validation/Angry/3996.jpg')\n",
        "\n",
        "cv2_imshow(image)"
      ],
      "metadata": {
        "id": "8HzqecGIxUpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Flatten, Activation\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pydot_ng\n",
        "import graphviz\n",
        "import os\n",
        "\n",
        "num_classes = 6\n",
        "\n",
        "Img_Height = 48\n",
        "Img_width = 48\n",
        "\n",
        "batch_size = 32\n",
        "train_directory = \"drive/MyDrive/probe/train\"\n",
        "train_dir = os.listdir(train_directory)\n",
        "validation_directory = \"drive/MyDrive/probe/validation\"\n",
        "validation_dir = os.listdir(validation_directory)\n",
        "\n",
        "#train_dir = \"train\"\n",
        "#validation_dir = \"validation\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=60,\n",
        "                                   shear_range=0.5,\n",
        "                                   zoom_range=0.5,\n",
        "                                   width_shift_range=0.5,\n",
        "                                   height_shift_range=0.5,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_directory,\n",
        "                                                    color_mode='grayscale',\n",
        "                                                    target_size=(Img_Height, Img_width),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=True)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_directory,\n",
        "                                                              color_mode='grayscale',\n",
        "                                                              target_size=(Img_Height, Img_width),\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              shuffle=True)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Block-1: The First Convolutional Block\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal',\n",
        "                 activation=\"elu\", \n",
        "                 input_shape=(Img_Height, Img_width, 1), \n",
        "                 name=\"Conv1\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm1\"))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal', \n",
        "                 activation=\"elu\", name=\"Conv2\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm2\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool1\"))\n",
        "model.add(Dropout(0.2, name=\"Dropout1\"))\n",
        "\n",
        "# Block-2: The Second Convolutional Block\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal',\n",
        "                 activation=\"elu\", name=\"Conv3\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm3\"))\n",
        "\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding='same',\n",
        "                 kernel_initializer='he_normal', \n",
        "                 activation=\"elu\", name=\"Conv4\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm4\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool2\"))\n",
        "model.add(Dropout(0.2, name=\"Dropout2\"))\n",
        "\n",
        "# Block-3: The Third Convolutional Block\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal', \n",
        "                 activation=\"elu\", name=\"Conv5\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm5\"))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal',\n",
        "                 activation=\"elu\", name=\"Conv6\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm6\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool3\"))\n",
        "model.add(Dropout(0.2, name=\"Dropout3\"))\n",
        "\n",
        "# Block-4: The Fully Connected Block\n",
        "\n",
        "model.add(Flatten(name=\"Flatten\"))\n",
        "model.add(Dense(64, activation=\"elu\", kernel_initializer='he_normal', name=\"Dense\"))\n",
        "model.add(BatchNormalization(name=\"Batch_Norm7\"))\n",
        "model.add(Dropout(0.5, name=\"Dropout4\"))\n",
        "\n",
        "# Block-5: The Output Block\n",
        "\n",
        "model.add(Dense(num_classes, activation=\"softmax\", kernel_initializer='he_normal', name = \"Output\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"emotions.h5\", monitor='accuracy', verbose=1,\n",
        "                              save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='accuracy', factor=0.2, patience=10, \n",
        "                           min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logs'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir, histogram_freq=False)\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = Adam(lr = 0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "train_samples = 28353\n",
        "validation_samples = 3534\n",
        "epochs = 150\n",
        "batch_size = 64\n",
        "\n",
        "model.fit(train_generator,\n",
        "          steps_per_epoch = train_samples//batch_size,\n",
        "          epochs = epochs,\n",
        "          callbacks = [checkpoint, reduce, tensorboard_Visualization],\n",
        "          validation_data = validation_generator,\n",
        "          validation_steps = validation_samples//batch_size,\n",
        "          shuffle=True)\n",
        "\n",
        "\n",
        "#model.save('drive/MyDrive/probe/saved_model/my_model') \n",
        "#model.save('drive/MyDrive/emotion.h5')\n",
        "#Sequential.save('drive/MyDrive/emotion.h5')\n",
        "\n",
        "\n",
        "model.save('drive/MyDrive/probe/saved_model/my_model')"
      ],
      "metadata": {
        "id": "VC9j-cT5PIay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5a5cc2-3177-49fe-c8dd-1d9efb354115"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 28273 images belonging to 6 classes.\n",
            "Found 7067 images belonging to 6 classes.\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 2.1184 - accuracy: 0.1888\n",
            "Epoch 00001: accuracy improved from -inf to 0.18883, saving model to emotions.h5\n",
            "443/443 [==============================] - 676s 2s/step - loss: 2.1184 - accuracy: 0.1888 - val_loss: 1.7445 - val_accuracy: 0.2585 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.8215 - accuracy: 0.2251\n",
            "Epoch 00002: accuracy improved from 0.18883 to 0.22505, saving model to emotions.h5\n",
            "443/443 [==============================] - 523s 1s/step - loss: 1.8215 - accuracy: 0.2251 - val_loss: 1.7168 - val_accuracy: 0.2648 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7732 - accuracy: 0.2423\n",
            "Epoch 00003: accuracy improved from 0.22505 to 0.24229, saving model to emotions.h5\n",
            "443/443 [==============================] - 440s 992ms/step - loss: 1.7732 - accuracy: 0.2423 - val_loss: 1.7203 - val_accuracy: 0.2648 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7455 - accuracy: 0.2538\n",
            "Epoch 00004: accuracy improved from 0.24229 to 0.25381, saving model to emotions.h5\n",
            "443/443 [==============================] - 435s 983ms/step - loss: 1.7455 - accuracy: 0.2538 - val_loss: 1.7242 - val_accuracy: 0.2585 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7497 - accuracy: 0.2508\n",
            "Epoch 00005: accuracy did not improve from 0.25381\n",
            "443/443 [==============================] - 367s 828ms/step - loss: 1.7497 - accuracy: 0.2508 - val_loss: 1.7195 - val_accuracy: 0.2670 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7341 - accuracy: 0.2604\n",
            "Epoch 00006: accuracy improved from 0.25381 to 0.26044, saving model to emotions.h5\n",
            "443/443 [==============================] - 374s 845ms/step - loss: 1.7341 - accuracy: 0.2604 - val_loss: 1.7230 - val_accuracy: 0.2551 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7430 - accuracy: 0.2547\n",
            "Epoch 00007: accuracy did not improve from 0.26044\n",
            "443/443 [==============================] - 337s 760ms/step - loss: 1.7430 - accuracy: 0.2547 - val_loss: 1.7178 - val_accuracy: 0.2744 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7388 - accuracy: 0.2527\n",
            "Epoch 00008: accuracy did not improve from 0.26044\n",
            "443/443 [==============================] - 373s 842ms/step - loss: 1.7388 - accuracy: 0.2527 - val_loss: 1.7138 - val_accuracy: 0.2841 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7367 - accuracy: 0.2579\n",
            "Epoch 00009: accuracy did not improve from 0.26044\n",
            "443/443 [==============================] - 336s 759ms/step - loss: 1.7367 - accuracy: 0.2579 - val_loss: 1.7244 - val_accuracy: 0.2744 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7390 - accuracy: 0.2566\n",
            "Epoch 00010: accuracy did not improve from 0.26044\n",
            "443/443 [==============================] - 337s 760ms/step - loss: 1.7390 - accuracy: 0.2566 - val_loss: 1.7273 - val_accuracy: 0.2693 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7358 - accuracy: 0.2592\n",
            "Epoch 00011: accuracy did not improve from 0.26044\n",
            "443/443 [==============================] - 317s 714ms/step - loss: 1.7358 - accuracy: 0.2592 - val_loss: 1.7227 - val_accuracy: 0.2756 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7392 - accuracy: 0.2540\n",
            "Epoch 00012: accuracy did not improve from 0.26044\n",
            "443/443 [==============================] - 317s 716ms/step - loss: 1.7392 - accuracy: 0.2540 - val_loss: 1.7169 - val_accuracy: 0.2619 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7369 - accuracy: 0.2587\n",
            "Epoch 00013: accuracy did not improve from 0.26044\n",
            "443/443 [==============================] - 313s 707ms/step - loss: 1.7369 - accuracy: 0.2587 - val_loss: 1.7152 - val_accuracy: 0.2744 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7341 - accuracy: 0.2600\n",
            "Epoch 00014: accuracy did not improve from 0.26044\n",
            "443/443 [==============================] - 313s 706ms/step - loss: 1.7341 - accuracy: 0.2600 - val_loss: 1.7268 - val_accuracy: 0.2517 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7322 - accuracy: 0.2598\n",
            "Epoch 00015: accuracy did not improve from 0.26044\n",
            "443/443 [==============================] - 308s 694ms/step - loss: 1.7322 - accuracy: 0.2598 - val_loss: 1.7708 - val_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7334 - accuracy: 0.2602\n",
            "Epoch 00016: accuracy did not improve from 0.26044\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "443/443 [==============================] - 326s 735ms/step - loss: 1.7334 - accuracy: 0.2602 - val_loss: 1.6724 - val_accuracy: 0.3034 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7189 - accuracy: 0.2647\n",
            "Epoch 00017: accuracy improved from 0.26044 to 0.26467, saving model to emotions.h5\n",
            "443/443 [==============================] - 315s 712ms/step - loss: 1.7189 - accuracy: 0.2647 - val_loss: 1.6900 - val_accuracy: 0.3063 - lr: 2.0000e-04\n",
            "Epoch 18/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7097 - accuracy: 0.2740\n",
            "Epoch 00018: accuracy improved from 0.26467 to 0.27399, saving model to emotions.h5\n",
            "443/443 [==============================] - 313s 706ms/step - loss: 1.7097 - accuracy: 0.2740 - val_loss: 1.6609 - val_accuracy: 0.2937 - lr: 2.0000e-04\n",
            "Epoch 19/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7007 - accuracy: 0.2739\n",
            "Epoch 00019: accuracy did not improve from 0.27399\n",
            "443/443 [==============================] - 318s 718ms/step - loss: 1.7007 - accuracy: 0.2739 - val_loss: 1.6973 - val_accuracy: 0.2966 - lr: 2.0000e-04\n",
            "Epoch 20/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6990 - accuracy: 0.2779\n",
            "Epoch 00020: accuracy improved from 0.27399 to 0.27795, saving model to emotions.h5\n",
            "443/443 [==============================] - 316s 714ms/step - loss: 1.6990 - accuracy: 0.2779 - val_loss: 1.6468 - val_accuracy: 0.3153 - lr: 2.0000e-04\n",
            "Epoch 21/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6897 - accuracy: 0.2842\n",
            "Epoch 00021: accuracy improved from 0.27795 to 0.28421, saving model to emotions.h5\n",
            "443/443 [==============================] - 306s 690ms/step - loss: 1.6897 - accuracy: 0.2842 - val_loss: 1.6240 - val_accuracy: 0.3341 - lr: 2.0000e-04\n",
            "Epoch 22/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6881 - accuracy: 0.2854\n",
            "Epoch 00022: accuracy improved from 0.28421 to 0.28543, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 695ms/step - loss: 1.6881 - accuracy: 0.2854 - val_loss: 1.6173 - val_accuracy: 0.3318 - lr: 2.0000e-04\n",
            "Epoch 23/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6824 - accuracy: 0.2884\n",
            "Epoch 00023: accuracy improved from 0.28543 to 0.28837, saving model to emotions.h5\n",
            "443/443 [==============================] - 320s 723ms/step - loss: 1.6824 - accuracy: 0.2884 - val_loss: 1.6419 - val_accuracy: 0.3290 - lr: 2.0000e-04\n",
            "Epoch 24/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6729 - accuracy: 0.3014\n",
            "Epoch 00024: accuracy improved from 0.28837 to 0.30139, saving model to emotions.h5\n",
            "443/443 [==============================] - 309s 697ms/step - loss: 1.6729 - accuracy: 0.3014 - val_loss: 1.6347 - val_accuracy: 0.3545 - lr: 2.0000e-04\n",
            "Epoch 25/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6639 - accuracy: 0.3020\n",
            "Epoch 00025: accuracy improved from 0.30139 to 0.30203, saving model to emotions.h5\n",
            "443/443 [==============================] - 311s 701ms/step - loss: 1.6639 - accuracy: 0.3020 - val_loss: 1.6053 - val_accuracy: 0.3341 - lr: 2.0000e-04\n",
            "Epoch 26/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6626 - accuracy: 0.3047\n",
            "Epoch 00026: accuracy improved from 0.30203 to 0.30467, saving model to emotions.h5\n",
            "443/443 [==============================] - 309s 697ms/step - loss: 1.6626 - accuracy: 0.3047 - val_loss: 1.6798 - val_accuracy: 0.3420 - lr: 2.0000e-04\n",
            "Epoch 27/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6557 - accuracy: 0.3071\n",
            "Epoch 00027: accuracy improved from 0.30467 to 0.30707, saving model to emotions.h5\n",
            "443/443 [==============================] - 318s 719ms/step - loss: 1.6557 - accuracy: 0.3071 - val_loss: 1.5896 - val_accuracy: 0.3597 - lr: 2.0000e-04\n",
            "Epoch 28/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6483 - accuracy: 0.3090\n",
            "Epoch 00028: accuracy improved from 0.30707 to 0.30902, saving model to emotions.h5\n",
            "443/443 [==============================] - 315s 712ms/step - loss: 1.6483 - accuracy: 0.3090 - val_loss: 1.7004 - val_accuracy: 0.3420 - lr: 2.0000e-04\n",
            "Epoch 29/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6442 - accuracy: 0.3085\n",
            "Epoch 00029: accuracy did not improve from 0.30902\n",
            "443/443 [==============================] - 307s 693ms/step - loss: 1.6442 - accuracy: 0.3085 - val_loss: 1.6548 - val_accuracy: 0.3341 - lr: 2.0000e-04\n",
            "Epoch 30/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6347 - accuracy: 0.3152\n",
            "Epoch 00030: accuracy improved from 0.30902 to 0.31518, saving model to emotions.h5\n",
            "443/443 [==============================] - 305s 688ms/step - loss: 1.6347 - accuracy: 0.3152 - val_loss: 1.6153 - val_accuracy: 0.3562 - lr: 2.0000e-04\n",
            "Epoch 31/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6251 - accuracy: 0.3224\n",
            "Epoch 00031: accuracy improved from 0.31518 to 0.32243, saving model to emotions.h5\n",
            "443/443 [==============================] - 311s 702ms/step - loss: 1.6251 - accuracy: 0.3224 - val_loss: 1.5057 - val_accuracy: 0.3938 - lr: 2.0000e-04\n",
            "Epoch 32/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6177 - accuracy: 0.3277\n",
            "Epoch 00032: accuracy improved from 0.32243 to 0.32774, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 695ms/step - loss: 1.6177 - accuracy: 0.3277 - val_loss: 1.6423 - val_accuracy: 0.3585 - lr: 2.0000e-04\n",
            "Epoch 33/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6067 - accuracy: 0.3364\n",
            "Epoch 00033: accuracy improved from 0.32774 to 0.33642, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 695ms/step - loss: 1.6067 - accuracy: 0.3364 - val_loss: 1.5797 - val_accuracy: 0.3864 - lr: 2.0000e-04\n",
            "Epoch 34/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6057 - accuracy: 0.3399\n",
            "Epoch 00034: accuracy improved from 0.33642 to 0.33995, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 694ms/step - loss: 1.6057 - accuracy: 0.3399 - val_loss: 1.4455 - val_accuracy: 0.4290 - lr: 2.0000e-04\n",
            "Epoch 35/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5928 - accuracy: 0.3411\n",
            "Epoch 00035: accuracy improved from 0.33995 to 0.34108, saving model to emotions.h5\n",
            "443/443 [==============================] - 320s 723ms/step - loss: 1.5928 - accuracy: 0.3411 - val_loss: 1.4921 - val_accuracy: 0.4051 - lr: 2.0000e-04\n",
            "Epoch 36/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5911 - accuracy: 0.3481\n",
            "Epoch 00036: accuracy improved from 0.34108 to 0.34807, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 696ms/step - loss: 1.5911 - accuracy: 0.3481 - val_loss: 1.4574 - val_accuracy: 0.4233 - lr: 2.0000e-04\n",
            "Epoch 37/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5895 - accuracy: 0.3460\n",
            "Epoch 00037: accuracy did not improve from 0.34807\n",
            "443/443 [==============================] - 311s 701ms/step - loss: 1.5895 - accuracy: 0.3460 - val_loss: 1.4484 - val_accuracy: 0.4273 - lr: 2.0000e-04\n",
            "Epoch 38/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5842 - accuracy: 0.3493\n",
            "Epoch 00038: accuracy improved from 0.34807 to 0.34932, saving model to emotions.h5\n",
            "443/443 [==============================] - 311s 703ms/step - loss: 1.5842 - accuracy: 0.3493 - val_loss: 1.4683 - val_accuracy: 0.4239 - lr: 2.0000e-04\n",
            "Epoch 39/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5711 - accuracy: 0.3580\n",
            "Epoch 00039: accuracy improved from 0.34932 to 0.35800, saving model to emotions.h5\n",
            "443/443 [==============================] - 323s 730ms/step - loss: 1.5711 - accuracy: 0.3580 - val_loss: 1.4591 - val_accuracy: 0.4392 - lr: 2.0000e-04\n",
            "Epoch 40/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5662 - accuracy: 0.3599\n",
            "Epoch 00040: accuracy improved from 0.35800 to 0.35993, saving model to emotions.h5\n",
            "443/443 [==============================] - 313s 705ms/step - loss: 1.5662 - accuracy: 0.3599 - val_loss: 1.4256 - val_accuracy: 0.4403 - lr: 2.0000e-04\n",
            "Epoch 41/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5704 - accuracy: 0.3592\n",
            "Epoch 00041: accuracy did not improve from 0.35993\n",
            "443/443 [==============================] - 311s 701ms/step - loss: 1.5704 - accuracy: 0.3592 - val_loss: 1.3343 - val_accuracy: 0.4659 - lr: 2.0000e-04\n",
            "Epoch 42/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5584 - accuracy: 0.3607\n",
            "Epoch 00042: accuracy improved from 0.35993 to 0.36071, saving model to emotions.h5\n",
            "443/443 [==============================] - 310s 700ms/step - loss: 1.5584 - accuracy: 0.3607 - val_loss: 1.3733 - val_accuracy: 0.4648 - lr: 2.0000e-04\n",
            "Epoch 43/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5673 - accuracy: 0.3638\n",
            "Epoch 00043: accuracy improved from 0.36071 to 0.36382, saving model to emotions.h5\n",
            "443/443 [==============================] - 320s 722ms/step - loss: 1.5673 - accuracy: 0.3638 - val_loss: 1.3275 - val_accuracy: 0.4795 - lr: 2.0000e-04\n",
            "Epoch 44/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5459 - accuracy: 0.3716\n",
            "Epoch 00044: accuracy improved from 0.36382 to 0.37158, saving model to emotions.h5\n",
            "443/443 [==============================] - 322s 726ms/step - loss: 1.5459 - accuracy: 0.3716 - val_loss: 1.3880 - val_accuracy: 0.4585 - lr: 2.0000e-04\n",
            "Epoch 45/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5528 - accuracy: 0.3680\n",
            "Epoch 00045: accuracy did not improve from 0.37158\n",
            "443/443 [==============================] - 309s 697ms/step - loss: 1.5528 - accuracy: 0.3680 - val_loss: 1.3578 - val_accuracy: 0.4528 - lr: 2.0000e-04\n",
            "Epoch 46/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5494 - accuracy: 0.3721\n",
            "Epoch 00046: accuracy improved from 0.37158 to 0.37208, saving model to emotions.h5\n",
            "443/443 [==============================] - 311s 702ms/step - loss: 1.5494 - accuracy: 0.3721 - val_loss: 1.3915 - val_accuracy: 0.4364 - lr: 2.0000e-04\n",
            "Epoch 47/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5421 - accuracy: 0.3738\n",
            "Epoch 00047: accuracy improved from 0.37208 to 0.37384, saving model to emotions.h5\n",
            "443/443 [==============================] - 321s 723ms/step - loss: 1.5421 - accuracy: 0.3738 - val_loss: 1.3307 - val_accuracy: 0.4653 - lr: 2.0000e-04\n",
            "Epoch 48/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5359 - accuracy: 0.3791\n",
            "Epoch 00048: accuracy improved from 0.37384 to 0.37914, saving model to emotions.h5\n",
            "443/443 [==============================] - 309s 698ms/step - loss: 1.5359 - accuracy: 0.3791 - val_loss: 1.3859 - val_accuracy: 0.4625 - lr: 2.0000e-04\n",
            "Epoch 49/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5226 - accuracy: 0.3807\n",
            "Epoch 00049: accuracy improved from 0.37914 to 0.38069, saving model to emotions.h5\n",
            "443/443 [==============================] - 319s 719ms/step - loss: 1.5226 - accuracy: 0.3807 - val_loss: 1.2899 - val_accuracy: 0.4915 - lr: 2.0000e-04\n",
            "Epoch 50/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.3807\n",
            "Epoch 00050: accuracy improved from 0.38069 to 0.38071, saving model to emotions.h5\n",
            "443/443 [==============================] - 311s 701ms/step - loss: 1.5291 - accuracy: 0.3807 - val_loss: 1.3213 - val_accuracy: 0.4818 - lr: 2.0000e-04\n",
            "Epoch 51/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5217 - accuracy: 0.3864\n",
            "Epoch 00051: accuracy improved from 0.38071 to 0.38636, saving model to emotions.h5\n",
            "443/443 [==============================] - 319s 719ms/step - loss: 1.5217 - accuracy: 0.3864 - val_loss: 1.2929 - val_accuracy: 0.4966 - lr: 2.0000e-04\n",
            "Epoch 52/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5279 - accuracy: 0.3838\n",
            "Epoch 00052: accuracy did not improve from 0.38636\n",
            "443/443 [==============================] - 309s 698ms/step - loss: 1.5279 - accuracy: 0.3838 - val_loss: 1.3260 - val_accuracy: 0.4756 - lr: 2.0000e-04\n",
            "Epoch 53/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5192 - accuracy: 0.3870\n",
            "Epoch 00053: accuracy improved from 0.38636 to 0.38699, saving model to emotions.h5\n",
            "443/443 [==============================] - 322s 728ms/step - loss: 1.5192 - accuracy: 0.3870 - val_loss: 1.3232 - val_accuracy: 0.4830 - lr: 2.0000e-04\n",
            "Epoch 54/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5101 - accuracy: 0.3927\n",
            "Epoch 00054: accuracy improved from 0.38699 to 0.39271, saving model to emotions.h5\n",
            "443/443 [==============================] - 311s 701ms/step - loss: 1.5101 - accuracy: 0.3927 - val_loss: 1.3534 - val_accuracy: 0.4847 - lr: 2.0000e-04\n",
            "Epoch 55/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5080 - accuracy: 0.3926\n",
            "Epoch 00055: accuracy did not improve from 0.39271\n",
            "443/443 [==============================] - 308s 694ms/step - loss: 1.5080 - accuracy: 0.3926 - val_loss: 1.2763 - val_accuracy: 0.4824 - lr: 2.0000e-04\n",
            "Epoch 56/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5145 - accuracy: 0.3906\n",
            "Epoch 00056: accuracy did not improve from 0.39271\n",
            "443/443 [==============================] - 309s 698ms/step - loss: 1.5145 - accuracy: 0.3906 - val_loss: 1.2265 - val_accuracy: 0.5284 - lr: 2.0000e-04\n",
            "Epoch 57/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5150 - accuracy: 0.3857\n",
            "Epoch 00057: accuracy did not improve from 0.39271\n",
            "443/443 [==============================] - 311s 703ms/step - loss: 1.5150 - accuracy: 0.3857 - val_loss: 1.2527 - val_accuracy: 0.5091 - lr: 2.0000e-04\n",
            "Epoch 58/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4941 - accuracy: 0.3960\n",
            "Epoch 00058: accuracy improved from 0.39271 to 0.39602, saving model to emotions.h5\n",
            "443/443 [==============================] - 325s 734ms/step - loss: 1.4941 - accuracy: 0.3960 - val_loss: 1.3168 - val_accuracy: 0.4733 - lr: 2.0000e-04\n",
            "Epoch 59/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5014 - accuracy: 0.3928\n",
            "Epoch 00059: accuracy did not improve from 0.39602\n",
            "443/443 [==============================] - 323s 730ms/step - loss: 1.5014 - accuracy: 0.3928 - val_loss: 1.2619 - val_accuracy: 0.5097 - lr: 2.0000e-04\n",
            "Epoch 60/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5059 - accuracy: 0.3946\n",
            "Epoch 00060: accuracy did not improve from 0.39602\n",
            "443/443 [==============================] - 315s 711ms/step - loss: 1.5059 - accuracy: 0.3946 - val_loss: 1.2364 - val_accuracy: 0.4983 - lr: 2.0000e-04\n",
            "Epoch 61/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4944 - accuracy: 0.3987\n",
            "Epoch 00061: accuracy improved from 0.39602 to 0.39870, saving model to emotions.h5\n",
            "443/443 [==============================] - 312s 705ms/step - loss: 1.4944 - accuracy: 0.3987 - val_loss: 1.2589 - val_accuracy: 0.5097 - lr: 2.0000e-04\n",
            "Epoch 62/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4897 - accuracy: 0.4043\n",
            "Epoch 00062: accuracy improved from 0.39870 to 0.40428, saving model to emotions.h5\n",
            "443/443 [==============================] - 323s 729ms/step - loss: 1.4897 - accuracy: 0.4043 - val_loss: 1.2672 - val_accuracy: 0.4909 - lr: 2.0000e-04\n",
            "Epoch 63/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4939 - accuracy: 0.4025\n",
            "Epoch 00063: accuracy did not improve from 0.40428\n",
            "443/443 [==============================] - 313s 706ms/step - loss: 1.4939 - accuracy: 0.4025 - val_loss: 1.2039 - val_accuracy: 0.5205 - lr: 2.0000e-04\n",
            "Epoch 64/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4877 - accuracy: 0.4024\n",
            "Epoch 00064: accuracy did not improve from 0.40428\n",
            "443/443 [==============================] - 310s 701ms/step - loss: 1.4877 - accuracy: 0.4024 - val_loss: 1.2467 - val_accuracy: 0.5159 - lr: 2.0000e-04\n",
            "Epoch 65/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4893 - accuracy: 0.3969\n",
            "Epoch 00065: accuracy did not improve from 0.40428\n",
            "443/443 [==============================] - 308s 696ms/step - loss: 1.4893 - accuracy: 0.3969 - val_loss: 1.2773 - val_accuracy: 0.4932 - lr: 2.0000e-04\n",
            "Epoch 66/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4792 - accuracy: 0.4040\n",
            "Epoch 00066: accuracy did not improve from 0.40428\n",
            "443/443 [==============================] - 308s 695ms/step - loss: 1.4792 - accuracy: 0.4040 - val_loss: 1.3137 - val_accuracy: 0.4949 - lr: 2.0000e-04\n",
            "Epoch 67/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4807 - accuracy: 0.4061\n",
            "Epoch 00067: accuracy improved from 0.40428 to 0.40611, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 695ms/step - loss: 1.4807 - accuracy: 0.4061 - val_loss: 1.3091 - val_accuracy: 0.4773 - lr: 2.0000e-04\n",
            "Epoch 68/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4737 - accuracy: 0.4103\n",
            "Epoch 00068: accuracy improved from 0.40611 to 0.41028, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 695ms/step - loss: 1.4737 - accuracy: 0.4103 - val_loss: 1.1909 - val_accuracy: 0.5398 - lr: 2.0000e-04\n",
            "Epoch 69/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4800 - accuracy: 0.4018\n",
            "Epoch 00069: accuracy did not improve from 0.41028\n",
            "443/443 [==============================] - 306s 691ms/step - loss: 1.4800 - accuracy: 0.4018 - val_loss: 1.2526 - val_accuracy: 0.5011 - lr: 2.0000e-04\n",
            "Epoch 70/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4783 - accuracy: 0.4065\n",
            "Epoch 00070: accuracy did not improve from 0.41028\n",
            "443/443 [==============================] - 305s 688ms/step - loss: 1.4783 - accuracy: 0.4065 - val_loss: 1.2165 - val_accuracy: 0.5193 - lr: 2.0000e-04\n",
            "Epoch 71/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4767 - accuracy: 0.4129\n",
            "Epoch 00071: accuracy improved from 0.41028 to 0.41288, saving model to emotions.h5\n",
            "443/443 [==============================] - 307s 693ms/step - loss: 1.4767 - accuracy: 0.4129 - val_loss: 1.2427 - val_accuracy: 0.4920 - lr: 2.0000e-04\n",
            "Epoch 72/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4745 - accuracy: 0.4046\n",
            "Epoch 00072: accuracy did not improve from 0.41288\n",
            "443/443 [==============================] - 307s 694ms/step - loss: 1.4745 - accuracy: 0.4046 - val_loss: 1.2789 - val_accuracy: 0.5074 - lr: 2.0000e-04\n",
            "Epoch 73/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4634 - accuracy: 0.4117\n",
            "Epoch 00073: accuracy did not improve from 0.41288\n",
            "443/443 [==============================] - 320s 723ms/step - loss: 1.4634 - accuracy: 0.4117 - val_loss: 1.1878 - val_accuracy: 0.5290 - lr: 2.0000e-04\n",
            "Epoch 74/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4653 - accuracy: 0.4112\n",
            "Epoch 00074: accuracy did not improve from 0.41288\n",
            "443/443 [==============================] - 309s 698ms/step - loss: 1.4653 - accuracy: 0.4112 - val_loss: 1.2023 - val_accuracy: 0.5318 - lr: 2.0000e-04\n",
            "Epoch 75/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4666 - accuracy: 0.4093\n",
            "Epoch 00075: accuracy did not improve from 0.41288\n",
            "443/443 [==============================] - 311s 702ms/step - loss: 1.4666 - accuracy: 0.4093 - val_loss: 1.1965 - val_accuracy: 0.5210 - lr: 2.0000e-04\n",
            "Epoch 76/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4506 - accuracy: 0.4211\n",
            "Epoch 00076: accuracy improved from 0.41288 to 0.42109, saving model to emotions.h5\n",
            "443/443 [==============================] - 306s 691ms/step - loss: 1.4506 - accuracy: 0.4211 - val_loss: 1.2359 - val_accuracy: 0.5176 - lr: 2.0000e-04\n",
            "Epoch 77/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4639 - accuracy: 0.4131\n",
            "Epoch 00077: accuracy did not improve from 0.42109\n",
            "443/443 [==============================] - 309s 697ms/step - loss: 1.4639 - accuracy: 0.4131 - val_loss: 1.2491 - val_accuracy: 0.5011 - lr: 2.0000e-04\n",
            "Epoch 78/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4595 - accuracy: 0.4149\n",
            "Epoch 00078: accuracy did not improve from 0.42109\n",
            "443/443 [==============================] - 308s 694ms/step - loss: 1.4595 - accuracy: 0.4149 - val_loss: 1.1752 - val_accuracy: 0.5364 - lr: 2.0000e-04\n",
            "Epoch 79/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4481 - accuracy: 0.4181\n",
            "Epoch 00079: accuracy did not improve from 0.42109\n",
            "443/443 [==============================] - 309s 698ms/step - loss: 1.4481 - accuracy: 0.4181 - val_loss: 1.2185 - val_accuracy: 0.5341 - lr: 2.0000e-04\n",
            "Epoch 80/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4635 - accuracy: 0.4135\n",
            "Epoch 00080: accuracy did not improve from 0.42109\n",
            "443/443 [==============================] - 308s 696ms/step - loss: 1.4635 - accuracy: 0.4135 - val_loss: 1.1808 - val_accuracy: 0.5386 - lr: 2.0000e-04\n",
            "Epoch 81/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4491 - accuracy: 0.4217\n",
            "Epoch 00081: accuracy improved from 0.42109 to 0.42172, saving model to emotions.h5\n",
            "443/443 [==============================] - 311s 702ms/step - loss: 1.4491 - accuracy: 0.4217 - val_loss: 1.1743 - val_accuracy: 0.5523 - lr: 2.0000e-04\n",
            "Epoch 82/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4513 - accuracy: 0.4181\n",
            "Epoch 00082: accuracy did not improve from 0.42172\n",
            "443/443 [==============================] - 308s 696ms/step - loss: 1.4513 - accuracy: 0.4181 - val_loss: 1.1367 - val_accuracy: 0.5631 - lr: 2.0000e-04\n",
            "Epoch 83/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4463 - accuracy: 0.4241\n",
            "Epoch 00083: accuracy improved from 0.42172 to 0.42410, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 694ms/step - loss: 1.4463 - accuracy: 0.4241 - val_loss: 1.1714 - val_accuracy: 0.5301 - lr: 2.0000e-04\n",
            "Epoch 84/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4434 - accuracy: 0.4217\n",
            "Epoch 00084: accuracy did not improve from 0.42410\n",
            "443/443 [==============================] - 307s 693ms/step - loss: 1.4434 - accuracy: 0.4217 - val_loss: 1.2223 - val_accuracy: 0.5256 - lr: 2.0000e-04\n",
            "Epoch 85/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4458 - accuracy: 0.4215\n",
            "Epoch 00085: accuracy did not improve from 0.42410\n",
            "443/443 [==============================] - 310s 701ms/step - loss: 1.4458 - accuracy: 0.4215 - val_loss: 1.2386 - val_accuracy: 0.5051 - lr: 2.0000e-04\n",
            "Epoch 86/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4481 - accuracy: 0.4256\n",
            "Epoch 00086: accuracy improved from 0.42410 to 0.42561, saving model to emotions.h5\n",
            "443/443 [==============================] - 312s 704ms/step - loss: 1.4481 - accuracy: 0.4256 - val_loss: 1.1706 - val_accuracy: 0.5426 - lr: 2.0000e-04\n",
            "Epoch 87/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4387 - accuracy: 0.4260\n",
            "Epoch 00087: accuracy improved from 0.42561 to 0.42596, saving model to emotions.h5\n",
            "443/443 [==============================] - 310s 700ms/step - loss: 1.4387 - accuracy: 0.4260 - val_loss: 1.1878 - val_accuracy: 0.5318 - lr: 2.0000e-04\n",
            "Epoch 88/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4361 - accuracy: 0.4253\n",
            "Epoch 00088: accuracy did not improve from 0.42596\n",
            "443/443 [==============================] - 319s 720ms/step - loss: 1.4361 - accuracy: 0.4253 - val_loss: 1.1671 - val_accuracy: 0.5381 - lr: 2.0000e-04\n",
            "Epoch 89/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4395 - accuracy: 0.4233\n",
            "Epoch 00089: accuracy did not improve from 0.42596\n",
            "443/443 [==============================] - 318s 718ms/step - loss: 1.4395 - accuracy: 0.4233 - val_loss: 1.2115 - val_accuracy: 0.5261 - lr: 2.0000e-04\n",
            "Epoch 90/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4350 - accuracy: 0.4299\n",
            "Epoch 00090: accuracy improved from 0.42596 to 0.42991, saving model to emotions.h5\n",
            "443/443 [==============================] - 321s 724ms/step - loss: 1.4350 - accuracy: 0.4299 - val_loss: 1.2124 - val_accuracy: 0.5335 - lr: 2.0000e-04\n",
            "Epoch 91/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4358 - accuracy: 0.4293\n",
            "Epoch 00091: accuracy did not improve from 0.42991\n",
            "443/443 [==============================] - 307s 693ms/step - loss: 1.4358 - accuracy: 0.4293 - val_loss: 1.1980 - val_accuracy: 0.5386 - lr: 2.0000e-04\n",
            "Epoch 92/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4300 - accuracy: 0.4267\n",
            "Epoch 00092: accuracy did not improve from 0.42991\n",
            "443/443 [==============================] - 304s 685ms/step - loss: 1.4300 - accuracy: 0.4267 - val_loss: 1.1347 - val_accuracy: 0.5591 - lr: 2.0000e-04\n",
            "Epoch 93/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4311 - accuracy: 0.4314\n",
            "Epoch 00093: accuracy improved from 0.42991 to 0.43136, saving model to emotions.h5\n",
            "443/443 [==============================] - 304s 687ms/step - loss: 1.4311 - accuracy: 0.4314 - val_loss: 1.1872 - val_accuracy: 0.5375 - lr: 2.0000e-04\n",
            "Epoch 94/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4356 - accuracy: 0.4209\n",
            "Epoch 00094: accuracy did not improve from 0.43136\n",
            "443/443 [==============================] - 305s 688ms/step - loss: 1.4356 - accuracy: 0.4209 - val_loss: 1.2093 - val_accuracy: 0.5210 - lr: 2.0000e-04\n",
            "Epoch 95/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4189 - accuracy: 0.4319\n",
            "Epoch 00095: accuracy improved from 0.43136 to 0.43186, saving model to emotions.h5\n",
            "443/443 [==============================] - 304s 685ms/step - loss: 1.4189 - accuracy: 0.4319 - val_loss: 1.1516 - val_accuracy: 0.5506 - lr: 2.0000e-04\n",
            "Epoch 96/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4372 - accuracy: 0.4217\n",
            "Epoch 00096: accuracy did not improve from 0.43186\n",
            "443/443 [==============================] - 303s 684ms/step - loss: 1.4372 - accuracy: 0.4217 - val_loss: 1.1848 - val_accuracy: 0.5341 - lr: 2.0000e-04\n",
            "Epoch 97/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4153 - accuracy: 0.4312\n",
            "Epoch 00097: accuracy did not improve from 0.43186\n",
            "443/443 [==============================] - 304s 685ms/step - loss: 1.4153 - accuracy: 0.4312 - val_loss: 1.1545 - val_accuracy: 0.5506 - lr: 2.0000e-04\n",
            "Epoch 98/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4330 - accuracy: 0.4252\n",
            "Epoch 00098: accuracy did not improve from 0.43186\n",
            "443/443 [==============================] - 305s 688ms/step - loss: 1.4330 - accuracy: 0.4252 - val_loss: 1.1893 - val_accuracy: 0.5290 - lr: 2.0000e-04\n",
            "Epoch 99/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4158 - accuracy: 0.4343\n",
            "Epoch 00099: accuracy improved from 0.43186 to 0.43426, saving model to emotions.h5\n",
            "443/443 [==============================] - 306s 692ms/step - loss: 1.4158 - accuracy: 0.4343 - val_loss: 1.1962 - val_accuracy: 0.5341 - lr: 2.0000e-04\n",
            "Epoch 100/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4193 - accuracy: 0.4332\n",
            "Epoch 00100: accuracy did not improve from 0.43426\n",
            "443/443 [==============================] - 306s 690ms/step - loss: 1.4193 - accuracy: 0.4332 - val_loss: 1.1153 - val_accuracy: 0.5614 - lr: 2.0000e-04\n",
            "Epoch 101/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4243 - accuracy: 0.4338\n",
            "Epoch 00101: accuracy did not improve from 0.43426\n",
            "443/443 [==============================] - 306s 691ms/step - loss: 1.4243 - accuracy: 0.4338 - val_loss: 1.1795 - val_accuracy: 0.5420 - lr: 2.0000e-04\n",
            "Epoch 102/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4243 - accuracy: 0.4337\n",
            "Epoch 00102: accuracy did not improve from 0.43426\n",
            "443/443 [==============================] - 306s 691ms/step - loss: 1.4243 - accuracy: 0.4337 - val_loss: 1.1716 - val_accuracy: 0.5381 - lr: 2.0000e-04\n",
            "Epoch 103/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4144 - accuracy: 0.4369\n",
            "Epoch 00103: accuracy improved from 0.43426 to 0.43694, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 694ms/step - loss: 1.4144 - accuracy: 0.4369 - val_loss: 1.1478 - val_accuracy: 0.5528 - lr: 2.0000e-04\n",
            "Epoch 104/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4075 - accuracy: 0.4436\n",
            "Epoch 00104: accuracy improved from 0.43694 to 0.44357, saving model to emotions.h5\n",
            "443/443 [==============================] - 305s 688ms/step - loss: 1.4075 - accuracy: 0.4436 - val_loss: 1.1700 - val_accuracy: 0.5432 - lr: 2.0000e-04\n",
            "Epoch 105/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4197 - accuracy: 0.4368\n",
            "Epoch 00105: accuracy did not improve from 0.44357\n",
            "443/443 [==============================] - 304s 687ms/step - loss: 1.4197 - accuracy: 0.4368 - val_loss: 1.1631 - val_accuracy: 0.5432 - lr: 2.0000e-04\n",
            "Epoch 106/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4233 - accuracy: 0.4264\n",
            "Epoch 00106: accuracy did not improve from 0.44357\n",
            "443/443 [==============================] - 306s 690ms/step - loss: 1.4233 - accuracy: 0.4264 - val_loss: 1.1566 - val_accuracy: 0.5551 - lr: 2.0000e-04\n",
            "Epoch 107/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4051 - accuracy: 0.4432\n",
            "Epoch 00107: accuracy did not improve from 0.44357\n",
            "443/443 [==============================] - 305s 688ms/step - loss: 1.4051 - accuracy: 0.4432 - val_loss: 1.1629 - val_accuracy: 0.5398 - lr: 2.0000e-04\n",
            "Epoch 108/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4021 - accuracy: 0.4416\n",
            "Epoch 00108: accuracy did not improve from 0.44357\n",
            "443/443 [==============================] - 307s 692ms/step - loss: 1.4021 - accuracy: 0.4416 - val_loss: 1.1882 - val_accuracy: 0.5489 - lr: 2.0000e-04\n",
            "Epoch 109/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4120 - accuracy: 0.4395\n",
            "Epoch 00109: accuracy did not improve from 0.44357\n",
            "443/443 [==============================] - 306s 690ms/step - loss: 1.4120 - accuracy: 0.4395 - val_loss: 1.1336 - val_accuracy: 0.5659 - lr: 2.0000e-04\n",
            "Epoch 110/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4113 - accuracy: 0.4400\n",
            "Epoch 00110: accuracy did not improve from 0.44357\n",
            "443/443 [==============================] - 305s 688ms/step - loss: 1.4113 - accuracy: 0.4400 - val_loss: 1.1638 - val_accuracy: 0.5455 - lr: 2.0000e-04\n",
            "Epoch 111/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4062 - accuracy: 0.4393\n",
            "Epoch 00111: accuracy did not improve from 0.44357\n",
            "443/443 [==============================] - 315s 712ms/step - loss: 1.4062 - accuracy: 0.4393 - val_loss: 1.1447 - val_accuracy: 0.5472 - lr: 2.0000e-04\n",
            "Epoch 112/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4069 - accuracy: 0.4431\n",
            "Epoch 00112: accuracy did not improve from 0.44357\n",
            "443/443 [==============================] - 327s 738ms/step - loss: 1.4069 - accuracy: 0.4431 - val_loss: 1.1557 - val_accuracy: 0.5403 - lr: 2.0000e-04\n",
            "Epoch 113/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3958 - accuracy: 0.4496\n",
            "Epoch 00113: accuracy improved from 0.44357 to 0.44962, saving model to emotions.h5\n",
            "443/443 [==============================] - 308s 694ms/step - loss: 1.3958 - accuracy: 0.4496 - val_loss: 1.1207 - val_accuracy: 0.5540 - lr: 2.0000e-04\n",
            "Epoch 114/150\n",
            " 92/443 [=====>........................] - ETA: 3:51 - loss: 1.3756 - accuracy: 0.4582"
          ]
        }
      ]
    }
  ]
}