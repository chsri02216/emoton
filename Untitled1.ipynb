{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bLx505BiP7huKcxYXdB6NJdOyBmmF8KB",
      "authorship_tag": "ABX9TyOTX4bC3tDhwNq407ddMBBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chsri02216/emoton/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiTJ-8u5fUA5",
        "outputId": "48e97a44-4bc8-4172-b9d3-b91de8bba82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyHpccqGPC4S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "#/content/drive/MyDrive/probe/fer2013.csv\n",
        "data = pd.read_csv(\"drive/MyDrive/probe/fer2013.csv\")\n",
        "\n",
        "#data = pd.read_csv(\"https://drive.google.com/file/d/1PpaJ8vAvw_6lK0Tu8_0xwMtyrmNIAodc/view?usp=sharing.csv\")\n",
        "data.head()\n",
        "\n",
        "\n",
        "labels = []\n",
        "usage = []\n",
        "\n",
        "for i in data[\"emotion\"]:\n",
        "    labels.append(i)\n",
        "    \n",
        "for i in data[\"Usage\"]:\n",
        "    usage.append(i)\n",
        "    \n",
        "print(set(labels))\n",
        "print(set(usage))\n",
        "\n",
        "\n",
        "\n",
        "count0 = 0\n",
        "count1 = 0\n",
        "count2 = 0\n",
        "count3 = 0\n",
        "count4 = 0\n",
        "count5 = 0\n",
        "count6 = 0\n",
        "\n",
        "for i, j, k in tqdm(zip(data[\"emotion\"], data[\"pixels\"], data[\"Usage\"])):\n",
        "    pixel = []\n",
        "    #print(pixel)\n",
        "    pixels = j.split(' ')\n",
        "    for m in pixels:\n",
        "        value = float(m)\n",
        "        pixel.append(value)\n",
        "    pixel = np.array(pixel)\n",
        "    image = pixel.reshape(48, 48)\n",
        "    \n",
        "    if k == \"Training\":\n",
        "        if not os.path.exists(\"drive/MyDrive/probe/train\"):\n",
        "            os.mkdir(\"drive/MyDrive/probe/train\")\n",
        "        if i == 0:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Angry\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Angry\")\n",
        "                path = \"drive/MyDrive/probe/train/Angry/\" + str(count0) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count0 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Angry/\" + str(count0) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count0 += 1\n",
        "        if i == 1:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Disgust\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Disgust\")\n",
        "                path = \"drive/MyDrive/probe/train/Disgust/\" + str(count1) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count1 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Disgust/\" + str(count1) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count1 += 1\n",
        "        if i == 2:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Fear\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Fear\")\n",
        "                path = \"drive/MyDrive/probe/train/Fear/\" + str(count2) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count2 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Fear/\" + str(count2) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count2 += 1\n",
        "        if i == 3:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Happy\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Happy\")\n",
        "                path = \"drive/MyDrive/probe/train/Happy/\" + str(count3) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count3 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Happy/\" + str(count3) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count3 += 1\n",
        "        if i == 4:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Sad\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Sad\")\n",
        "                path = \"drive/MyDrive/probe/train/Sad/\" + str(count4) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count4 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Sad/\" + str(count4) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count4 += 1\n",
        "        if i == 5:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Surprise\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Surprise\")\n",
        "                path = \"drive/MyDrive/probe/train/Surprise/\" + str(count5) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count5 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Surprise/\" + str(count5) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count5 += 1\n",
        "        if i == 6:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/train/Neutral\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/train/Neutral\")\n",
        "                path = \"drive/MyDrive/probe/train/Neutral/\" + str(count6) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count6 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/train/Neutral/\" + str(count6) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count6 += 1\n",
        "    else:\n",
        "        if not os.path.exists(\"drive/MyDrive/probe/validation\"):\n",
        "            os.mkdir(\"drive/MyDrive/probe/validation\")\n",
        "        if i == 0:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Angry\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Angry\")\n",
        "                path = \"drive/MyDrive/probe/validation/Angry/\" + str(count0) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count0 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Angry/\" + str(count0) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count0 += 1\n",
        "        if i == 1:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Disgust\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Disgust\")\n",
        "                path = \"drive/MyDrive/probe/validation/Disgust/\" + str(count1) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count1 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Disgust/\" + str(count1) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count1 += 1\n",
        "        if i == 2:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Fear\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Fear\")\n",
        "                path = \"drive/MyDrive/probe/validation/Fear/\" + str(count2) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count2 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Fear/\" + str(count2) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count2 += 1\n",
        "        if i == 3:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Happy\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Happy\")\n",
        "                path = \"drive/MyDrive/probe/validation/Happy/\" + str(count3) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count3 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Happy/\" + str(count3) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count3 += 1\n",
        "        if i == 4:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Sad\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Sad\")\n",
        "                path = \"drive/MyDrive/probe/validation/Sad/\" + str(count4) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count4 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Sad/\" + str(count4) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count4 += 1\n",
        "        if i == 5:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Surprise\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Surprise\")\n",
        "                path = \"drive/MyDrive/probe/validation/Surprise/\" + str(count5) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count5 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Surprise/\" + str(count5) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count5 += 1\n",
        "        if i == 6:\n",
        "            if not os.path.exists(\"drive/MyDrive/probe/validation/Neutral\"):\n",
        "                os.mkdir(\"drive/MyDrive/probe/validation/Neutral\")\n",
        "                path = \"drive/MyDrive/probe/validation/Neutral/\" + str(count6) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count6 += 1\n",
        "            else:\n",
        "                path = \"drive/MyDrive/probe/validation/Neutral/\" + str(count6) + \".jpg\"\n",
        "                cv2.imwrite(path , image)\n",
        "                count6 += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_directory = \"drive/MyDrive/probe/train/\"\n",
        "train_dir = os.listdir(train_directory)\n",
        "classes = []\n",
        "\n",
        "for folder in train_dir:\n",
        "    classes.append(folder)\n",
        "\n",
        "train_counts = []\n",
        "\n",
        "for folder in train_dir:\n",
        "    class_path = train_directory + folder + \"/\"\n",
        "    list_train = []\n",
        "    count = 0\n",
        "    for file in os.listdir(class_path):\n",
        "        count +=1\n",
        "    \n",
        "    train_counts.append(count)\n",
        "    \n",
        "train_counts\n",
        "\n",
        "\n",
        "# plt.bar(classes, train_counts, width=0.5)\n",
        "# plt.title(\"Bar Graph of Train Data\")\n",
        "# plt.xlabel(\"Classes\")\n",
        "# plt.ylabel(\"Counts\")\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(classes, train_counts)\n",
        "plt.plot(classes, train_counts, '-o')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F_9hoOFmPHq7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6abe3804-ae90-4ccd-b5fa-3d8d40c93435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9bn48c+TjSRsSSDsCQFkFZQlEhbrLoJLwV0QBW9vuVat7e21itUWq7TSa63Wn629roAormxVK+JCRfawyKqAEJbIHvawZHl+f8w3MGBCtknOLM/79ZrXnPM9Z848Z+bM9znLd75HVBVjjDGRJ8rrAIwxxnjDEoAxxkQoSwDGGBOhLAEYY0yEsgRgjDERKsbrAM6mcePGmpGR4XUYxhgTUpYsWbJHVVPLmy+oE0BGRgbZ2dleh2GMMSFFRDZXZD47BWSMMRHKEoAxxkQoSwDGGBOhLAEYY0yEsgRgjDERKqhbARljas6j01YyeeFWilSJFmFoVhpjh3TzOixTiywBGBOBHp22kkkLtpwcL1I9OW5JIHLYKSBjItDkhVsrVW7CkyUAYyJQURn3ASmr3IQnSwDGRKBokUqVm/BkCcCYCHRpp9K7iRmalVbLkRgvlZsARKSjiCz3exwUkV+KSIqIzBKR9e452c0vIvKciGwQkRUi0tNvWSPc/OtFZERNrpgxpnQ7Dhxj2Zb9NIiPJspvhz8tOcEuAEeYclsBqeq3QHcAEYkGcoGpwGjgM1UdJyKj3fhDwCCgvXtkAS8AWSKSAowBMgEFlojIDFXdF/C1MsaU6kRhMfe8sYRjBUVMv+9CzmlSD4An/7WWl+dsInf/UVomJXgcpaktlT0FdDnwnapuBgYDE1z5BGCIGx4MTFSfBUCSiDQHrgJmqWqeq/RnAQOrvQbGmAp78l9rWbplP3+66byTlT/AHX1ao6pMWlChTiRNmKhsArgNmOyGm6rqdje8A2jqhlsC/m3JtrmyssqNMbXgn19/z2tzc7irfwbXntfitGmtkhMZ0KUZkxdt4VhBkUcRmtpW4QQgInHAj4F3z5ymqorvtE61icgoEckWkezdu3cHYpHGRLwNuw7x0Psr6NU6mYcHdS51npH9M9ifX8C0Zbm1HJ3xSmWOAAYBS1V1pxvf6U7t4J53ufJcwL8pQStXVlb5aVT1RVXNVNXM1NRyb2hjjCnHkeOF3D1pKQmx0fxtWE/iYkr/2We1SaFTs/qMn5eD2v8BIkJlEsBQTp3+AZgBlLTkGQFM9yu/07UG6gMccKeKZgIDRCTZtRga4MqMMTVEVRk9ZSUbdx/m/w3tQbOG8WXOKyLc1T+Db3YcYsHGvFqM0nilQglAROoCVwJT/IrHAVeKyHrgCjcO8BGwEdgAvATcA6CqecATwGL3eNyVGWNqyIR5Ofzz6+954KqO9DuncbnzD+7ekuTEWMbP21QL0RmvVagzOFU9AjQ6o2wvvlZBZ86rwL1lLOdV4NXKh2mMqawlm/cx9sO1XNG5KXdf1K5Cr4mPjWZo73T+8e/v2JqXT1pKYg1Habxk/wQ2JgztOXyce99YSoukBJ6+5XyioirexcPwPq0REWsSGgEsARgTZoqKlfsnL2Nf/gleGN6ThgmxlXp9i6QEBp7raxKaf6KwhqI0wcASgDFh5i+zvmXed3sZO6Qr57ZoWKVljOyfwcFjhUxb9n2AozPBxBKAMWHk0zU7+dsX3zG0dxo3Z1a9Y7fM1smc26IB4+dtsiahYcwSgDFhYsvefP77neV0bdmAMdedW61liQgj+2Wwbudh5n23N0ARmmBjCcCYMHCsoIi7Jy0hSoQXbu9FfGx0tZd53fktSKkbx2tzc6ofoAlKlgCMCQO/m76KNdsP8uyt3QPWdDM+NpphvdP57JudbNmbH5BlmuBiCcCYEPf24i28k72N+y87h0s7NQnosof3aU20CBPn5wR0uSY4WAIwJoStyj3Ab6ev5kftG/OLKzoEfPnNGsYzqFtz3s7eypHj1iQ03FgCMCZE7c8/wd2TltCobhx/va0H0ZX4s1dljOyXwaFjhUyxXkLDjiUAY0JQcbHyq3e+ZufBY/zt9p6k1I2rsffqmZ7Eea0aMn6uNQkNN5YAjAlBf5+9gc+/2cVvr+1Cz/TkGn2vkiah3+0+wlcb9tToe5naZQnAmBDz1fo9PD1rHYO7t+COPq1r5T2vOa85jetZk9BwYwnAmBCy/cBR7n9rGe2b1OPJG7ohUjPn/c9UJyaaYVmt+fybXWzac6RW3tPUPEsAxoSIE4XF3PPGUo4XFPHC8F4kxlWoN/eAGZ6VTmy0NQkNJ5YAjAkRf/xoLcu27Oepm8+nXWq9Wn//Jg3iuaZbc97N3sZhaxIaFiwBGBMCpi/PZfy8HP7zwjZc3a25Z3GM7N+Gw8cLeX/JNs9iMIFjCcCYILdu5yFGv7+SCzKSeWhQJ09j6Z6WRPe0JCbMy6G42JqEhjpLAMYEscPHC7l70hLq1onh+WE9iY32/id7V/8MNu45wpfrd3sdiqkm77cmY0ypVJWH3lvB5r35PD+sB00bxHsdEgCDujYntX4dxs/L8ToUU02WAIwJUq/OzeHDldt58KqO9GnbyOtwToqLiWJ4Vmtmf7ub73Yf9jocUw0VSgAikiQi74nINyKyVkT6ikiKiMwSkfXuOdnNKyLynIhsEJEVItLTbzkj3PzrRWRETa2UMaFucU4eT360lgFdmjLqorZeh/MDw7LSiYuOYqIdBYS0ih4B/BX4WFU7AecDa4HRwGeq2h74zI0DDALau8co4AUAEUkBxgBZQG9gTEnSMMacsvvQce59YymtkhP48y3n19qfvSojtX4drj2/Oe8t2cbBYwVeh2OqqNwEICINgYuAVwBU9YSq7gcGAxPcbBOAIW54MDBRfRYASSLSHLgKmKWqeaq6D5gFDAzo2hgT4gqLivn55KUcPFbAC8N70SA+1uuQynRXvzYcOVHEe9nWJDRUVeQIoA2wG3hNRJaJyMsiUhdoqqrb3Tw7gKZuuCWw1e/121xZWeWnEZFRIpItItm7d1srAxNZ/vzJOhZszOMPQ7rRuXkDr8M5q26tGtKrdTIT5luT0FBVkQQQA/QEXlDVHsARTp3uAUB9fcQGZAtQ1RdVNVNVM1NTUwOxSGNCwszVO/jHv79jWFY6N/Zq5XU4FTKyXwab9+Yze90ur0MxVVCRBLAN2KaqC934e/gSwk53agf3XLIF5AJpfq9v5crKKjcm4uXsOcID73zNea0a8rtru3gdToUN7NqMpg3qWC+hIarcBKCqO4CtItLRFV0OrAFmACUteUYA093wDOBO1xqoD3DAnSqaCQwQkWR38XeAKzMmoh09UcTdk5YQHS38bVhP4mOjvQ6pwmKjo7ijT2vmrN/Dhl2HvA7HVFJFWwH9HHhDRFYA3YE/AuOAK0VkPXCFGwf4CNgIbABeAu4BUNU84AlgsXs87sqMiViqyqPTVvHtzkM8e2t30lISvQ6p0ob2TicuJsr+GBaCKtSfrKouBzJLmXR5KfMqcG8Zy3kVeLUyARoTzt5avJX3l27jF5e355KOTbwOp0oa1avD4PNb8P6SXH59VScaJgRvyyVzOvsnsDEeWbFtP2Omr+aiDqncf3l7r8OplhH9MjhaUMS72VvLn9kEDUsAxnhgf/4JfjZpKan16/Dsrd2Jjgq+P3tVRteWDemdkcKE+TkUWZPQkGEJwJhaVlys/PLt5ew+dJy/396TlLpxXocUECP7Z7A17yiff2NNQkOFJQBjatnzX2xg9re7+d11XTg/LcnrcAJmQJemNG8Yz/h5m7wOxVSQJQBjatGX63bzzKfruKFHS27PSvc6nICKiY7ijr6tmbthL+t2WpPQUGAJwJhakrv/KL94axkdm9bnD9d3C8pO3qrrtgvSqWNNQkOGJQBjasHxwiLueWMphUXKC8N7kRAXOn/2qoyUunEM6d6SKUu3sT//hNfhmHJYAjCmFoz9YC1fb93PUzefT5vGdb0Op0aN7J/BsYJi3l5sTUKDnSUAY2rY1GXbeH3BZv7rorYM7NrM63BqXOfmDejTNoWJ8zdTWFTsdTjmLCwBGFODvtlxkIenrKR3mxR+fVXH8l8QJkb2a0Pu/qN8utaahAYzSwDG1JBDxwr42aSl1I+P5flhPYiJjpyf2xWdm9AyKcGahAa5yNkijalFqsqv313Blrx8nh/agyb1470OqVbFREdxZ9/WLNiYx9rtB70Ox5TBEoAxNeDlOZv4ePUORg/sRFbbRl6H44lbL0gjPjaKCdYkNGhZAjAmwBZu3Mu4j79h4LnN+M8ftfE6HM8kJcZxfY9WTF2Wy74j1iQ0GFkCMCaAdh08xn2Tl5GekshTN58Xln/2qoy7+mdwvLCYyYu3eB2KKYUlAGMCpLComPsmL+PwsUL+MbwX9eOtX/wOTevT/5xGvG5NQoOSJQBjAuSpmd+yaFMeT97QjY7N6nsdTtAY2a8N2w8c45M1O70OxZzBEoAxAfDxqu3835cbuaNPa4b0aOl1OEHlsk5NSEtJYLzdOD7oWAIwppo27j7MA++u4Py0JB69trPX4QSd6ChhRN8MFuXksSr3gNfhGD+WAIyphvwThfxs0lJio4W/396TOjHh2clbdd2cmUZCbLQ1CQ0yFUoAIpIjIitFZLmIZLuyFBGZJSLr3XOyKxcReU5ENojIChHp6becEW7+9SIyomZWyZjaoao8MnUV63Yd4q+39aBlUoLXIQWthgmx3NirJdO//p69h497HY5xKnMEcKmqdlfVTDc+GvhMVdsDn7lxgEFAe/cYBbwAvoQBjAGygN7AmJKkYUwoeXTaSto9/BFtHv6Iqcty6dayARd1SPU6rKA3sl8GJwqLect6CQ0a1TkFNBiY4IYnAEP8yieqzwIgSUSaA1cBs1Q1T1X3AbOAgdV4f2Nq3aPTVjJpwRaK9NSNz1dsO8ij01Z6GFVoOKdJfX7UvjGvz99MgTUJDQoxFZxPgU9ERIH/U9UXgaaqut1N3wE0dcMtAf8Uv82VlVV+GhEZhe/IgfT08LplngluxwqK2Jd/grwjJ9h3pIC8/BPsO3KCfe45L7+Af379famvnbxwK2OHdKvliEPPXf0z+I/x2Xy8agfXnd/C63AiXkUTwIWqmisiTYBZIvKN/0RVVZccqs0llxcBMjMzA7JME3mOFxaxP7+g1Ao9z1XqeScrd998+SeKylxeg/gYUurGlTnd/4jAlO2SDk1o3SiR8fNyLAEEgQolAFXNdc+7RGQqvnP4O0Wkuapud6d4Sjr+zgXS/F7eypXlApecUT67WtGbkPHotJVMXriVIlWiRRialVbhPeaCouIzKvMT5OWfYH9+wWnjp54LOHy8sMzl1a8TQ3LdOJLrxpFarw4dmtYnJdE3npwYR0rdWPfsK0tKiD3ZlXO7hz8qtbKPjvAuHyoqyjUJffyDNazYtp/zWiV5HVJEKzcBiEhdIEpVD7nhAcDjwAxgBDDOPU93L5kB3Ccib+G74HvAJYmZwB/9LvwOAB4O6NqYoFRy3rxEkSqTFmxh98Fj3NAr7fQK/Mipin6/ez54rOzKvG5cNMl1XWWdGEfb1HokJcaerNBTTlbqcSTXjSUpIY64mKpf+hqalXbauviXm4q5KbMVT3/yLePn5fCXW7p7HU5Eq8gRQFNgquvUKgZ4U1U/FpHFwDsi8hNgM3CLm/8j4GpgA5AP3AWgqnki8gSw2M33uKrmBWxNTNB6c2HpHYHNXLOLmWtO3TEqITb6ZEWdnBhHekqiXwUe66vQE+NIchV6UmIs8bG12+6+5KilqkczBhrEx3JTr1ZMXrSVhwd1JrV+Ha9DiliiQXzuMjMzU7Ozs70Ow1RRzp4jvL5gM698VfZdoT74+YUnK/mEOPsTVaT4bvdhLn/63/zqyg7cf3l7r8MJOyKyxK/JfpkqehHYmAopLlb+vW43E+bnMPvb3cRECYKvGdmZokXo2rJhLUdogkG71Hpc3CGVSQs2c/fF7ap1Ws5UnX3qJiAO5Bfw8pyNXPLn2dw1fjFrvj/IL69oz7zRl3F7n9Kb89p588h2V/8Mdh06zr9WbS9/ZlMj7AjAVMua7w8ycX4O05bncqygmN4ZKTw4sCNXnduMWNdyxs6bm9Jc1D6Vto3r8trcHAZ3tx5UvWAJwFTaicJiZq7ewcT5OSzO2Ud8bBTX92jJHX0y6NKiQamvGTukm1X45jRRUcKIfhmMmbGaZVv20SPdeoapbZYATIXtOniMNxdt4c2FW9h16DjpKYk8ek1nbu6VRsNEu/uVqbwbe7XiqZnfMmFejiUAD1gCMGelqmRv3seEeTl8vGoHhcXKJR1T+VPfDC7ukEpUlP0BylRdvTox3JzZikkLNvObqzvTpEG81yFFFEsAplRHTxQxfXkuE+ZvZu32gzSIj2FkvwyG92lNRuO6XodnwsiIvhmMn5fDGwu38N9XdvA6nIhiCcCcZvPeI0xasJm3F2/l4LFCOjWrz5M3dGNw9xYkxtnmYgIvo3FdLu3YhDcWbuGeS9vZTXVqkf2ija/t/vrdTJyXw+x1u4kW4aquzRjRN4MLMpIR6+fG1LC7+mdwxyuL+Gjldq7v0crrcCKGJYAIdiC/gHeXbGXSgs3k7M0ntX4d7r+sPcOy0mlq52JNLbrwnMac06Qer83NYUj3lrbTUUssAUSgtdsPMnH+ZqYty+VoQRGZrZP51YCODDy3mf0j03hCxNck9LfTVrF0y356tbYWQbXBEkCEKChybffnbWZRTh51YqIY0r0ld/Rtbd0xmKBwQ4+W/O/H3zB+Xo4lgFpiCSDM7Tp0jMkLt/Lmos3sPHictJQEHrm6MzdntiIpsewbnBhT2+rWieHWzDTGz8thx9WdadbQTkPWNEsAYUhVWbplHxPmbeZfq7ZTUKRc3CGVJ29ozcUdmhBtbfdNkLqzbwavzN3EGws38z8DOnodTtizBBBGjp4oYsbXuUycv5nV3x+kfnwMd/TJ4I6+rWljbfdNCEhvlMjlnZry5sIt3HvpObV+v4dIYwkgDGzZm8+khb62+weOFtCxaX3+cH1XhnRvSd069hWb0HJX/ww+XbuTD1Zs56Ze1iS0JlntEKKKi5U5G/YwcV4On3+7iygRBp7bjDv7tqZ3mxRrRmdCVr92jejQtB6vzd3EjT2tSWhNsgQQxEq7kfqvr+rEe0u28fr8HHL25tO4Xhw/v/Qchmal07xhgtchG1NtIsLIfm34zdSVZG/exwUZKV6HFLbslpBB6swbqZeIFihS6JmexIh+GQzs2sz+Om/CTv6JQvo++TkXntOYv93e0+twQo7dEjIEFBQVc+hYIYeOFXDoWCEH3fOhY4W8UcaN1IsV/nnfhXRrZW33TfhKjIvhtgvSePmrTXy//ygtkuzotiZUOAGISDSQDeSq6rUi0gZ4C2gELAHuUNUTIlIHmAj0AvYCt6pqjlvGw8BPgCLgflWdGciVKVHaqZNA34zkRGHxyYq7pBI/6FeZ+1fsh46XVPD+0ws4VlBc6fdVsMrfRIThfVrz0pyNTFqwmQcHdvI6nLBUmSOAXwBrgZJbPv0JeEZV3xKRf+Cr2F9wz/tU9RwRuc3Nd6uIdAFuA84FWgCfikgHVS0K0LoAPzx1UqR6crwkCZRXeR/0q6TPrMxL5jteWH7lnRgXTf34GOrHx1I/PoaGCbG0Sk6gQUlZnZjTppc8N4iP5ZI/f0FxKWfnou2CmIkQaSmJXNmlKZMXbeH+y9tbk9AaUKEEICKtgGuAPwC/Et9l+cuAYW6WCcBj+BLAYDcM8B7wvJt/MPCWqh4HNonIBqA3MD8ga+JMXri11PJJC7bw8aqdVa+8E+NolZJYocq7fnwM9eJjTt4TtyqGZaWXeg3AbqRuIsnIfm2YuXonM5Z/zy0X2LYfaBU9AngWeBCo78YbAftVtdCNbwNK7urcEtgKoKqFInLAzd8SWOC3TP/XnCQio4BRAOnp6RVekRJFZ7mofWWXpq4C/2HFfVrlXSeGmGpU3oFgN1I3Bvq0TaFTs/q8Ni+HmzNbWZPQACs3AYjItcAuVV0iIpfUdECq+iLwIvhaAVX29dEipSaBaBGevCG0Kk+7kbqJdCLCXf0zeOj9lSzalEdW20ZehxRWKrKb2x/4sYjk4LvoexnwVyBJREoSSCsg1w3nAmkAbnpDfBeDT5aX8pqAKesUiZ06MSY0De7ekqTEWF6bm+N1KGGn3ASgqg+raitVzcB3EfdzVb0d+AK4yc02Apjuhme4cdz0z9X3Z4MZwG0iUse1IGoPLArYmjhjh3RjeJ/0kxdLo0UY3ifd9qSNCVHxsdEM7Z3OJ2t2sG1fvtfhhJXq/A/gIeAtERkLLANeceWvAK+7i7x5+JIGqrpaRN4B1gCFwL2BbgFUwk6dGBNehvdpzYtfbuT1BZt5eFBnr8MJG/ZPYGNMSLjnjSXM3bCXBQ9fTkKcNQk9m4r+E9ju/2eMCQkj+7XhwNECpi0P+KXDiGUJwBgTEi7ISKZL8waMn5tDMJ+5CCWWAIwxIaGkSei3Ow8xf+Ner8MJC5YAjDEh47rzW5BSN47x1iQ0ICwBGGNCRnxsNMN6pzNr7U625lmT0OqyBGCMCSnD+7QmSoSJ83O8DiXkWQIwxoSUZg3jGdS1GW8t3sqR44Xlv8CUyRKAMSbk3NU/g0PHCpm6zJqEVoclAGNMyOmZnky3lg0ZP8+ahFaHJQBjTMgpaRK6Yddh5m6wJqFVZQnAGBOSrjmvOY3rxTF+3iavQwlZlgCMMSGpTkw0w7Ja89k3u9i894jX4YQkSwDGmJA1PMvX9fuEeZu9DiUkWQIwxoSsJg3iuea85rybvZXD1iS00iwBGGNC2sh+GRw6XsiUpdu8DiXkVOeGMMYY47ke6cmk1I3ld9NX87vpq4kWYWhWmt0UqgLsCMAYE9IenbaSvCMFJ8eLVJm0YAuPTlvpYVShwRKAMSakTV64tVLl5hRLAMaYkFZUxj+Byyo3p1gCMMaEtGiRSpWbU8pNACISLyKLRORrEVktIr935W1EZKGIbBCRt0UkzpXXceMb3PQMv2U97Mq/FZGramqljDGRY2hWWqXKzSkVOQI4DlymqucD3YGBItIH+BPwjKqeA+wDfuLm/wmwz5U/4+ZDRLoAtwHnAgOBv4tIdCBXxhgTecYO6cbwPukn9/hL9vtvybQEUJ5yE4D6HHajse6hwGXAe658AjDEDQ9247jpl4uIuPK3VPW4qm4CNgC9A7IWxpiINnZIN7578mpyxl3D148NoEn9Oox+fyUFRcVehxbUKnQNQESiRWQ5sAuYBXwH7FfVkr/ebQNauuGWwFYAN/0A0Mi/vJTX+L/XKBHJFpHs3bt3V36NjDERrUF8LI8PPpc12w/yylfWUdzZVCgBqGqRqnYHWuHba+9UUwGp6ouqmqmqmampqTX1NsaYMDawa3MGdGnKM7PWWUdxZ1GpVkCquh/4AugLJIlIyT+JWwElt+bJBdIA3PSGwF7/8lJeY4wxAfX44K7ERUfxm6kr7aYxZahIK6BUEUlywwnAlcBafIngJjfbCGC6G57hxnHTP1ffpz8DuM21EmoDtAcWBWpFjDHGX7OG8Tw4qBNzN+xlylLb1yxNRY4AmgNfiMgKYDEwS1U/AB4CfiUiG/Cd43/Fzf8K0MiV/woYDaCqq4F3gDXAx8C9qloUyJUxxhh/t/dOJ7N1MmM/XMPew8e9DifoSDAfGmVmZmp2drbXYRhjQtj6nYe4+rk5XNOtOc/e1sPrcGqFiCxR1czy5rN/Ahtjwlr7pvX52SXnMG359/x7nbUs9GcJwBgT9u69tB3tUuvyyNSV5J+wG8eUsARgjAl7dWKiGXfjeWzbd5S/fLLO63CChiUAY0xEuCAjhWFZ6bw6dxMrtx3wOpygYAnAGBMxHhrYicb16jB6ygoKrZsISwDGmMjRMCGW3//4XFZ/b91EgCUAY0yEGdi1GVd2acozn65jy958r8PxlCUAY0xEERGeGNyVmCjrJsISgDEm4jRrGM9DAzvy1YY9Ed1NhCUAY0xEuj2rNb0ivJsISwDGmIgUFSU8eUM3Dh8vZOyHa70OxxOWAIwxEatD0/r87OJ2TF2WG5HdRFgCMMZEtHsuPYe2EdpNhCUAY0xEi4+NZtwNvm4inpkVWd1EWAIwxkS83m1SGNo7nVe+iqxuIiwBGGMMMHpQ5HUTYQnAGGM4vZuIV+dGRjcRlgCMMcYZ2LUZV3Ruyl9mRUY3EZYAjDHGERGeGHIuMVFRPDIt/LuJsARgjDF+mjdM4MGBHZmzfg9Tl4V3NxHlJgARSRORL0RkjYisFpFfuPIUEZklIuvdc7IrFxF5TkQ2iMgKEenpt6wRbv71IjKi5lbLGGOqbnhWa3qmJ/HEB+HdTURFjgAKgf9R1S5AH+BeEekCjAY+U9X2wGduHGAQ0N49RgEvgC9hAGOALKA3MKYkaRhjTDCJihLG3Xgeh48X8ocw7iai3ASgqttVdakbPgSsBVoCg4EJbrYJwBA3PBiYqD4LgCQRaQ5cBcxS1TxV3QfMAgYGdG2MMSZASrqJmLIsly/DtJuISl0DEJEMoAewEGiqqtvdpB1AUzfcEtjq97Jtrqys8jPfY5SIZItI9u7d4fmhG2NCw8luIqaFZzcRFU4AIlIPeB/4paoe9J+mvkvlAblcrqovqmqmqmampqYGYpHGGFMl8bHRPHl9N7bmHeXZT9d7HU7AVSgBiEgsvsr/DVWd4op3ulM7uOddrjwXSPN7eStXVla5McYEray2jRjaO42X52xkVW54dRNRkVZAArwCrFXVv/hNmgGUtOQZAUz3K7/TtQbqAxxwp4pmAgNEJNld/B3gyowxJqiNHtSZRvXq8ND74dVNREWOAPoDdwCXichy97gaGAdcKSLrgSvcOMBHwEZgA/AScA+AquYBTwCL3eNxV2aMMUHNv5uI1+bmeB1OwEgw/9MtMzNTs7OzvQ7DGGNQVX46MZu5G/byyX9fRFpKotchlUlElqhqZnnz2T+BjTGmAkSExwd3JUrgN1PDo5sISwDGGFNBLZISeHBgJ+as38O05aHfhsUSgDHGVMLwPiR7I+cAAA13SURBVK3pkZ7EEx+sJe/ICa/DqRZLAMYYUwnRUcK4G87j0LECxn64xutwqsUSgDHGVFLHZvW5++J2TFmay5z1odtjgSUAY4ypgnsvPYe2jevyyNRVHD1R5HU4VWIJwBhjqiA+Npo/3tCNLXn5PPvpOq/DqRJLAMYYU0V92jbitgvSePmrTSHZTYQlAGOMqYaHB3UmOTGO0VNCr5sISwDGGFMNDRN93USsyj3I+Hk5XodTKZYAjDGmmq7u1owrOjfh6U/WsTUv3+twKswSgDHGVJN/NxGPTFsVMt1EWAIwxpgAaJGUwK+v6siX63Yzffn3XodTIZYAjDEmQO7om0H3tCQe/2BNSHQTYQnAGGMCJDpKGHdjNw4eDY1uIiwBGGNMAHVq1uBkNxFfrd/jdThnZQnAGGMC7L7LzqFN47r8ZurKoO4mwhKAMcYEWHxsNH+83nUT8VnwdhNhCcAYY2pA33aNuDUzjZfnBG83EZYAjDGmhvzmal83EQ9PWRmU3USUmwBE5FUR2SUiq/zKUkRkloisd8/JrlxE5DkR2SAiK0Skp99rRrj514vIiJpZHWOMCR4NE2N57MddWJl7ICi7iajIEcB4YOAZZaOBz1S1PfCZGwcYBLR3j1HAC+BLGMAYIAvoDYwpSRrGGBPOrunWnMs7BWc3EeUmAFX9Esg7o3gwMMENTwCG+JVPVJ8FQJKINAeuAmapap6q7gNm8cOkYowxYUdEeHyIr5uIR4Osm4iqXgNoqqrb3fAOoKkbbgls9Ztvmysrq/wHRGSUiGSLSPbu3aF7qzVjjCnRMimBB67qyL/X7WbG18HTTUS1LwKrL50FLKWp6ouqmqmqmampqYFarDHGeOrOkm4i/rmGfUHSTURVE8BOd2oH97zLlecCaX7ztXJlZZUbY0xEKOkm4sDRAsZ+uNbrcICqJ4AZQElLnhHAdL/yO11roD7AAXeqaCYwQESS3cXfAa7MGGMiRqdmDfivi9vy/tJtQdFNREWagU4G5gMdRWSbiPwEGAdcKSLrgSvcOMBHwEZgA/AScA+AquYBTwCL3eNxV2aMMRHl55e1D5puIiSYrkifKTMzU7Ozs70OwxhjAmred3sY9tJC7r64HaMHdQr48kVkiapmljef/RPYGGNqWb92jbklsxUvzdnI6u+96ybCEoAxxnjA101ELA9PWUlRsTdnYiwBGGOMB5IS4xhz3bms2HaA1+Zu8iQGSwDGGOORa89rzmUedhNhCcAYYzwiIjwxpCviUTcRlgCMMcZDLZMSeGCAN91EWAIwxhiPjeiXwfkedBNhCcAYYzwWHSWMu8HXTcQfPqq9biJiau2djDHGlKlz8waMuqgtf5/9HVOWbqNYIVqEoVlpjB3SrUbe044AjDEmSOS50z8lfwsoUmXSgi08Om1ljbyfJQBjjAkS72ZvK7V88sKtpZZXlyUAY4wJEkVlNAMtq7y6LAEYY0yQiBapVHl1WQIwxpggMTQrrVLl1WWtgIwxJkiUtPaZvHArRao13grI7gdgjDFhxu4HYIwx5qwsARhjTISyBGCMMRHKEoAxxkQoSwDGGBOhgroVkIjsBjZXYxGNgT0BCsdL4bIeYOsSjMJlPcDWpURrVU0tb6agTgDVJSLZFWkKFezCZT3A1iUYhct6gK1LZdkpIGOMiVCWAIwxJkKFewJ40esAAiRc1gNsXYJRuKwH2LpUSlhfAzDGGFO2cD8CMMYYUwZLAMYYE6FCJgGIyBARURHp5HUsgSIiRSKy3O+R4XVM1SUij4jIahFZ4dYpq4KvyxCRVTUdn3svFZGn/cYfEJHHqrisJBG5p4qvzRGRxlV5bSnLOnzG+EgReT4Qy65pVd1mqvA+H4lIUk0s+4z3KfldrxaRr0Xkf0Qkyk3LFJHnaiGGDBEZVt58oXQ/gKHAV+55THUXJiIxqlpY7aiq56iqdg/UwrxeJxHpC1wL9FTV465yi/MqnrM4DtwgIk+qanX/NJQE3AP8/cwJXn8foaA620xFP18REXzXO6+uXrQVdvJ3LSJNgDeBBsAYVc0GaqOP+wxgmHvvMoXEEYCI1AMuBH4C3ObKLhGR2SLynoh8IyJvuC8aEbnalS0RkedE5ANX/piIvC4ic4HXReRLEenu9z5ficj5tb+Gp4hILxH5t4t9pog0d+U/FZHFbo/ifRFJdOXjReQfIrIQ+F8vYweaA3tU9TiAqu5R1e9F5Hcu9lUi8qLf99TLrc/XwL21GGchvhYW/33mBBFJdZ/vYvfo78ofE5EH/OZb5Y7YxgHt3B7fU267nCMiM4A1bt5p7vtcLSKjamH9zlyn60RkoYgsE5FPRaSp3zq9LiLzRWS9iPzUlV/ifhsfisi3bvuKEpH/EJFn/Zb7UxF5pprhlbXNnDw6cnvNs8+IueQ3PFJEpru6YL2IjHHzZbjYJwKrgLSSZYpIXbduX7vv8Vb3mlJ/e9WhqruAUcB94nOJX310sZw6+l8mIvXd5/x3V3/NEt9Ry01u/rI+kx8sB992+SNX9oPt3D/AoH8AtwOvuOF5QC/gEuAA0ApfIpuPL0nEA1uBNm7+ycAHbvgxYAmQ4MZHAM+64Q5Adi2vVxGw3D2mArFu/VLd9FuBV91wI7/XjQV+7obHAx8A0UHwPdVz67IO3x7xxa48xW+e14Hr3PAK4CI3/BSwqpbiPIxvjywHaAg8ADzmpr0JXOiG04G1ftvOA37LWIVvLyvDP263XR4p2f781x9IcK9r5MZzgMY1sC0tB7YAz7tpyZxq8fefwNN+6/S1i6ux+920cOtwDGgLRAOzgJvc9/sdEOtePw/oVkPbzMnPBsgEZvvF7P8bHglsBxr5fb6Z7nspBvr4vVeOW88bgZf8yhtylt9eVbavUsr2A03dZ1tSH/0T6O/3OcS4z/kjfHVaM2AfcFM5n0lpyzn5Pmd7hMopoKHAX93wW278A2CRqm4DEJHl+L70w8BGVd3k5p+MLwOXmKGqR93wu8BvReTXwH/gq0xr02mngESkK9AVmOV2kqPxbdwAXUVkLL5TDvWAmX7LeVdVi2on5LKp6mER6QX8CLgUeFtERgOHRORBIBFIAVaLyBwgSVW/dC9/HRhUi7EedHuH9wNH/SZdAXSRUzfhbiC+I9DKWOS3/QHcLyLXu+E0oD2wtwphn82Z29JIfJUE+HaS3nZ7tHGAf2zT3e/hqIh8AfTGV1ktUtWNblmT8SXF90Tkc+BaEVmLLxGsrE7QZ9lmzsb/NwwwS1X3ulin4NsRnAZsVtUFpbx+JfC0iPwJXyU5p5zfXk2ZC/xFRN4ApqjqNhG5EN/vuRjY4b6TqiynQgEEfQIQkRTgMqCbiCi+L0aBD/Gdyy1RRMXW50jJgKrmi8gsYDBwC74jCy8JsFpV+5YybTwwRFW/dj/uS/ymHSllfk+4RDQbmC0iK4H/As4DMlV1q/gutsZ7F+FpngWWAq/5lUXh22s85j+jiBRy+inTs63Dye9DRC7Bl1T6uu1tdjmvrQn/D/iLqs5w8TzmN+3MPwJpOeUvA78BvuH0z63KStlmRuA7TVfyeZ/5eZ25vZcVa6m/C1VdJyI9gauBsSLyGb4j8LJ+e9UiIm3x1U+7gM5+cYwTkQ9dHHNF5KpyFlXqZ1KF5ZwUCtcAbgJeV9XWqpqhqmn49mB+VMb83wJt5VSLmlvLWf7LwHPAYlXdF4B4q+NbIFV8F8YQkVgROddNqw9sF5FYfKfEgo6IdBSR9n5F3fGtE8Aetyd9E4Cq7gf2uz0e8GCdVDUPeAfftaUSnwA/LxmRU9eIcoCerqwn0MaVH8L33ZSlIbDPVf6dgD4BCb5yGgK5bnjEGdMGi0i8iDTCt1Ox2JX3FpE24mu9ciu+Bhio6kJ8RzHD8B1dV0sZ28xmfJ93yQ7ZjeUs5koRSRGRBGAIvj3is71nCyBfVSfhO/XYk7P/9qpMRFKBf+A7HadnTGunqitV9U/4PvdOLvYb3bWAklNGJXIo5TMpYznlbZdAaCSAofiys7/3XfkPuEPDe4CPRWQJvg/iQFkLV9UlwEECtDdTHap6Al8F+SfxXRhdDvRzk38LLMS3gXzjTYTlqgdMEJE1IrIC6IJvb/MlfOdmZ3KqggG4C/ibO31XsWPWwHsa33nhEvcDmeJrkrgGuNuVvw+kiMhq4D5856xxpx7muouJT5Wy/I+BGHfKZBxQ2imJmvYY8K77PZzZ6mkF8AW+uJ5Q1e9d+WLgeWAtvh0u/9/gO8DcAO0wlbXN/B74q4hk49t7PptF+L6fFcD76mtpczbdgEVuuxsDjC3nt1dZCe7i62rgU3w7Fb8vZb5fuu1mBVAA/MutxzZ8DQgm4TtCLam/yvpMSlvOCqBIfBe6y7wIHJZdQYhIPXduUYC/AetVtdTWCm5vYDbQyZ13MyYiuNNxh1X1z2eUX4Lvgve1ZbzuA+AZVf2sxoMsR8m1DlW9z+tYAsWv/mqEL7n1V9UdNfFeoXAEUBU/ddl9Nb7D3/8rbSYRuRPfXvUjVvkbc3bi+9PbOnwXnD2v/MPYB67+moPvqKxGKn8I0yMAY4wx5QvXIwBjjDHlsARgjDERyhKAMcZEKEsAxhgToSwBGGNMhPr/edWBYEa6578AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "image = cv2.imread('/content/drive/MyDrive/probe/validation/Angry/3996.jpg')\n",
        "\n",
        "cv2_imshow(image)"
      ],
      "metadata": {
        "id": "8HzqecGIxUpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Flatten, Activation\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pydot_ng\n",
        "import graphviz\n",
        "import os\n",
        "\n",
        "num_classes = 6\n",
        "\n",
        "Img_Height = 48\n",
        "Img_width = 48\n",
        "\n",
        "batch_size = 32\n",
        "train_directory = \"drive/MyDrive/probe/train\"\n",
        "train_dir = os.listdir(train_directory)\n",
        "validation_directory = \"drive/MyDrive/probe/validation\"\n",
        "validation_dir = os.listdir(validation_directory)\n",
        "\n",
        "#train_dir = \"train\"\n",
        "#validation_dir = \"validation\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=60,\n",
        "                                   shear_range=0.5,\n",
        "                                   zoom_range=0.5,\n",
        "                                   width_shift_range=0.5,\n",
        "                                   height_shift_range=0.5,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_directory,\n",
        "                                                    color_mode='grayscale',\n",
        "                                                    target_size=(Img_Height, Img_width),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=True)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_directory,\n",
        "                                                              color_mode='grayscale',\n",
        "                                                              target_size=(Img_Height, Img_width),\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              shuffle=True)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Block-1: The First Convolutional Block\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal',\n",
        "                 activation=\"elu\", \n",
        "                 input_shape=(Img_Height, Img_width, 1), \n",
        "                 name=\"Conv1\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm1\"))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal', \n",
        "                 activation=\"elu\", name=\"Conv2\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm2\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool1\"))\n",
        "model.add(Dropout(0.2, name=\"Dropout1\"))\n",
        "\n",
        "# Block-2: The Second Convolutional Block\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal',\n",
        "                 activation=\"elu\", name=\"Conv3\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm3\"))\n",
        "\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding='same',\n",
        "                 kernel_initializer='he_normal', \n",
        "                 activation=\"elu\", name=\"Conv4\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm4\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool2\"))\n",
        "model.add(Dropout(0.2, name=\"Dropout2\"))\n",
        "\n",
        "# Block-3: The Third Convolutional Block\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal', \n",
        "                 activation=\"elu\", name=\"Conv5\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm5\"))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n",
        "                 kernel_initializer='he_normal',\n",
        "                 activation=\"elu\", name=\"Conv6\"))\n",
        "\n",
        "model.add(BatchNormalization(name=\"Batch_Norm6\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool3\"))\n",
        "model.add(Dropout(0.2, name=\"Dropout3\"))\n",
        "\n",
        "# Block-4: The Fully Connected Block\n",
        "\n",
        "model.add(Flatten(name=\"Flatten\"))\n",
        "model.add(Dense(64, activation=\"elu\", kernel_initializer='he_normal', name=\"Dense\"))\n",
        "model.add(BatchNormalization(name=\"Batch_Norm7\"))\n",
        "model.add(Dropout(0.5, name=\"Dropout4\"))\n",
        "\n",
        "# Block-5: The Output Block\n",
        "\n",
        "model.add(Dense(num_classes, activation=\"softmax\", kernel_initializer='he_normal', name = \"Output\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"emotions.h5\", monitor='accuracy', verbose=1,\n",
        "                              save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='accuracy', factor=0.2, patience=10, \n",
        "                           min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logs'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir, histogram_freq=False)\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = Adam(lr = 0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "train_samples = 28353\n",
        "validation_samples = 3534\n",
        "epochs = 150\n",
        "batch_size = 64\n",
        "\n",
        "model.fit(train_generator,\n",
        "          steps_per_epoch = train_samples//batch_size,\n",
        "          epochs = epochs,\n",
        "          callbacks = [checkpoint, reduce, tensorboard_Visualization],\n",
        "          validation_data = validation_generator,\n",
        "          validation_steps = validation_samples//batch_size,\n",
        "          shuffle=True)\n",
        "\n",
        "\n",
        "#model.save('drive/MyDrive/probe/saved_model/my_model') \n",
        "#model.save('drive/MyDrive/emotion.h5')\n",
        "#Sequential.save('drive/MyDrive/emotion.h5')\n",
        "\n",
        "\n",
        "model.save('drive/MyDrive/probe/saved_model/my_model')"
      ],
      "metadata": {
        "id": "VC9j-cT5PIay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379cbb21-4c2f-48f9-89a5-cb339f76c9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 28273 images belonging to 6 classes.\n",
            "Found 7067 images belonging to 6 classes.\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "443/443 [==============================] - ETA: 0s - loss: 2.0544 - accuracy: 0.1970\n",
            "Epoch 00001: accuracy improved from -inf to 0.19702, saving model to emotions.h5\n",
            "443/443 [==============================] - 336s 755ms/step - loss: 2.0544 - accuracy: 0.1970 - val_loss: 1.7798 - val_accuracy: 0.2222 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.8080 - accuracy: 0.2306\n",
            "Epoch 00002: accuracy improved from 0.19702 to 0.23056, saving model to emotions.h5\n",
            "443/443 [==============================] - 327s 738ms/step - loss: 1.8080 - accuracy: 0.2306 - val_loss: 1.7361 - val_accuracy: 0.2534 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7572 - accuracy: 0.2414\n",
            "Epoch 00003: accuracy improved from 0.23056 to 0.24137, saving model to emotions.h5\n",
            "443/443 [==============================] - 321s 725ms/step - loss: 1.7572 - accuracy: 0.2414 - val_loss: 1.7306 - val_accuracy: 0.2494 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7513 - accuracy: 0.2489\n",
            "Epoch 00004: accuracy improved from 0.24137 to 0.24885, saving model to emotions.h5\n",
            "443/443 [==============================] - 319s 721ms/step - loss: 1.7513 - accuracy: 0.2489 - val_loss: 1.7155 - val_accuracy: 0.2750 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7393 - accuracy: 0.2531\n",
            "Epoch 00005: accuracy improved from 0.24885 to 0.25309, saving model to emotions.h5\n",
            "443/443 [==============================] - 330s 745ms/step - loss: 1.7393 - accuracy: 0.2531 - val_loss: 1.7297 - val_accuracy: 0.2523 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7380 - accuracy: 0.2587\n",
            "Epoch 00006: accuracy improved from 0.25309 to 0.25874, saving model to emotions.h5\n",
            "443/443 [==============================] - 335s 757ms/step - loss: 1.7380 - accuracy: 0.2587 - val_loss: 1.7222 - val_accuracy: 0.2614 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7400 - accuracy: 0.2568\n",
            "Epoch 00007: accuracy did not improve from 0.25874\n",
            "443/443 [==============================] - 332s 749ms/step - loss: 1.7400 - accuracy: 0.2568 - val_loss: 1.7140 - val_accuracy: 0.2710 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7400 - accuracy: 0.2489\n",
            "Epoch 00008: accuracy did not improve from 0.25874\n",
            "443/443 [==============================] - 333s 751ms/step - loss: 1.7400 - accuracy: 0.2489 - val_loss: 1.7307 - val_accuracy: 0.2545 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7426 - accuracy: 0.2590\n",
            "Epoch 00009: accuracy improved from 0.25874 to 0.25896, saving model to emotions.h5\n",
            "443/443 [==============================] - 345s 779ms/step - loss: 1.7426 - accuracy: 0.2590 - val_loss: 1.7297 - val_accuracy: 0.2699 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7527 - accuracy: 0.2520\n",
            "Epoch 00010: accuracy did not improve from 0.25896\n",
            "443/443 [==============================] - 342s 773ms/step - loss: 1.7527 - accuracy: 0.2520 - val_loss: 1.7217 - val_accuracy: 0.2631 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7408 - accuracy: 0.2505\n",
            "Epoch 00011: accuracy did not improve from 0.25896\n",
            "443/443 [==============================] - 343s 775ms/step - loss: 1.7408 - accuracy: 0.2505 - val_loss: 1.7236 - val_accuracy: 0.2585 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7335 - accuracy: 0.2587\n",
            "Epoch 00012: accuracy did not improve from 0.25896\n",
            "443/443 [==============================] - 327s 737ms/step - loss: 1.7335 - accuracy: 0.2587 - val_loss: 1.7221 - val_accuracy: 0.2557 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7335 - accuracy: 0.2621\n",
            "Epoch 00013: accuracy improved from 0.25896 to 0.26213, saving model to emotions.h5\n",
            "443/443 [==============================] - 340s 768ms/step - loss: 1.7335 - accuracy: 0.2621 - val_loss: 1.7104 - val_accuracy: 0.2773 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7287 - accuracy: 0.2652\n",
            "Epoch 00014: accuracy improved from 0.26213 to 0.26524, saving model to emotions.h5\n",
            "443/443 [==============================] - 333s 752ms/step - loss: 1.7287 - accuracy: 0.2652 - val_loss: 1.7009 - val_accuracy: 0.2682 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7187 - accuracy: 0.2657\n",
            "Epoch 00015: accuracy improved from 0.26524 to 0.26573, saving model to emotions.h5\n",
            "443/443 [==============================] - 347s 783ms/step - loss: 1.7187 - accuracy: 0.2657 - val_loss: 1.8695 - val_accuracy: 0.2489 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.7069 - accuracy: 0.2700\n",
            "Epoch 00016: accuracy improved from 0.26573 to 0.27004, saving model to emotions.h5\n",
            "443/443 [==============================] - 345s 778ms/step - loss: 1.7069 - accuracy: 0.2700 - val_loss: 1.6373 - val_accuracy: 0.3074 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6985 - accuracy: 0.2772\n",
            "Epoch 00017: accuracy improved from 0.27004 to 0.27723, saving model to emotions.h5\n",
            "443/443 [==============================] - 335s 756ms/step - loss: 1.6985 - accuracy: 0.2772 - val_loss: 1.7285 - val_accuracy: 0.3057 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6878 - accuracy: 0.2857\n",
            "Epoch 00018: accuracy improved from 0.27723 to 0.28571, saving model to emotions.h5\n",
            "443/443 [==============================] - 351s 792ms/step - loss: 1.6878 - accuracy: 0.2857 - val_loss: 1.6126 - val_accuracy: 0.3375 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6804 - accuracy: 0.2907\n",
            "Epoch 00019: accuracy improved from 0.28571 to 0.29066, saving model to emotions.h5\n",
            "443/443 [==============================] - 338s 763ms/step - loss: 1.6804 - accuracy: 0.2907 - val_loss: 1.6358 - val_accuracy: 0.3278 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6617 - accuracy: 0.3051\n",
            "Epoch 00020: accuracy improved from 0.29066 to 0.30509, saving model to emotions.h5\n",
            "443/443 [==============================] - 365s 825ms/step - loss: 1.6617 - accuracy: 0.3051 - val_loss: 1.7315 - val_accuracy: 0.2699 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6466 - accuracy: 0.3124\n",
            "Epoch 00021: accuracy improved from 0.30509 to 0.31236, saving model to emotions.h5\n",
            "443/443 [==============================] - 334s 755ms/step - loss: 1.6466 - accuracy: 0.3124 - val_loss: 1.4945 - val_accuracy: 0.4034 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6264 - accuracy: 0.3218\n",
            "Epoch 00022: accuracy improved from 0.31236 to 0.32180, saving model to emotions.h5\n",
            "443/443 [==============================] - 332s 749ms/step - loss: 1.6264 - accuracy: 0.3218 - val_loss: 1.6324 - val_accuracy: 0.3659 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.6104 - accuracy: 0.3367\n",
            "Epoch 00023: accuracy improved from 0.32180 to 0.33670, saving model to emotions.h5\n",
            "443/443 [==============================] - 321s 724ms/step - loss: 1.6104 - accuracy: 0.3367 - val_loss: 1.4652 - val_accuracy: 0.4403 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5869 - accuracy: 0.3465\n",
            "Epoch 00024: accuracy improved from 0.33670 to 0.34650, saving model to emotions.h5\n",
            "443/443 [==============================] - 331s 746ms/step - loss: 1.5869 - accuracy: 0.3465 - val_loss: 1.4342 - val_accuracy: 0.4347 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5782 - accuracy: 0.3546\n",
            "Epoch 00025: accuracy improved from 0.34650 to 0.35461, saving model to emotions.h5\n",
            "443/443 [==============================] - 324s 731ms/step - loss: 1.5782 - accuracy: 0.3546 - val_loss: 1.4035 - val_accuracy: 0.4580 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5747 - accuracy: 0.3572\n",
            "Epoch 00026: accuracy improved from 0.35461 to 0.35718, saving model to emotions.h5\n",
            "443/443 [==============================] - 323s 729ms/step - loss: 1.5747 - accuracy: 0.3572 - val_loss: 1.3894 - val_accuracy: 0.4574 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5543 - accuracy: 0.3703\n",
            "Epoch 00027: accuracy improved from 0.35718 to 0.37034, saving model to emotions.h5\n",
            "443/443 [==============================] - 360s 813ms/step - loss: 1.5543 - accuracy: 0.3703 - val_loss: 1.3978 - val_accuracy: 0.4443 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5372 - accuracy: 0.3739\n",
            "Epoch 00028: accuracy improved from 0.37034 to 0.37394, saving model to emotions.h5\n",
            "443/443 [==============================] - 368s 831ms/step - loss: 1.5372 - accuracy: 0.3739 - val_loss: 1.2824 - val_accuracy: 0.4915 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5332 - accuracy: 0.3797\n",
            "Epoch 00029: accuracy improved from 0.37394 to 0.37973, saving model to emotions.h5\n",
            "443/443 [==============================] - 368s 831ms/step - loss: 1.5332 - accuracy: 0.3797 - val_loss: 1.3125 - val_accuracy: 0.4773 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5259 - accuracy: 0.3813\n",
            "Epoch 00030: accuracy improved from 0.37973 to 0.38135, saving model to emotions.h5\n",
            "443/443 [==============================] - 355s 800ms/step - loss: 1.5259 - accuracy: 0.3813 - val_loss: 1.3988 - val_accuracy: 0.4437 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5157 - accuracy: 0.3870\n",
            "Epoch 00031: accuracy improved from 0.38135 to 0.38699, saving model to emotions.h5\n",
            "443/443 [==============================] - 361s 814ms/step - loss: 1.5157 - accuracy: 0.3870 - val_loss: 1.3189 - val_accuracy: 0.4773 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5107 - accuracy: 0.3922\n",
            "Epoch 00032: accuracy improved from 0.38699 to 0.39220, saving model to emotions.h5\n",
            "443/443 [==============================] - 370s 834ms/step - loss: 1.5107 - accuracy: 0.3922 - val_loss: 1.3513 - val_accuracy: 0.4693 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5133 - accuracy: 0.3867\n",
            "Epoch 00033: accuracy did not improve from 0.39220\n",
            "443/443 [==============================] - 369s 833ms/step - loss: 1.5133 - accuracy: 0.3867 - val_loss: 1.3536 - val_accuracy: 0.4636 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.5068 - accuracy: 0.3907\n",
            "Epoch 00034: accuracy did not improve from 0.39220\n",
            "443/443 [==============================] - 363s 818ms/step - loss: 1.5068 - accuracy: 0.3907 - val_loss: 1.3942 - val_accuracy: 0.4631 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4950 - accuracy: 0.3957\n",
            "Epoch 00035: accuracy improved from 0.39220 to 0.39566, saving model to emotions.h5\n",
            "443/443 [==============================] - 359s 811ms/step - loss: 1.4950 - accuracy: 0.3957 - val_loss: 1.3139 - val_accuracy: 0.4869 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4917 - accuracy: 0.3996\n",
            "Epoch 00036: accuracy improved from 0.39566 to 0.39962, saving model to emotions.h5\n",
            "443/443 [==============================] - 355s 802ms/step - loss: 1.4917 - accuracy: 0.3996 - val_loss: 1.2555 - val_accuracy: 0.5040 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4867 - accuracy: 0.4016\n",
            "Epoch 00037: accuracy improved from 0.39962 to 0.40160, saving model to emotions.h5\n",
            "443/443 [==============================] - 341s 770ms/step - loss: 1.4867 - accuracy: 0.4016 - val_loss: 1.2592 - val_accuracy: 0.4932 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4864 - accuracy: 0.4035\n",
            "Epoch 00038: accuracy improved from 0.40160 to 0.40350, saving model to emotions.h5\n",
            "443/443 [==============================] - 374s 844ms/step - loss: 1.4864 - accuracy: 0.4035 - val_loss: 1.2994 - val_accuracy: 0.4841 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4727 - accuracy: 0.4089\n",
            "Epoch 00039: accuracy improved from 0.40350 to 0.40887, saving model to emotions.h5\n",
            "443/443 [==============================] - 370s 835ms/step - loss: 1.4727 - accuracy: 0.4089 - val_loss: 1.4452 - val_accuracy: 0.4403 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4752 - accuracy: 0.4070\n",
            "Epoch 00040: accuracy did not improve from 0.40887\n",
            "443/443 [==============================] - 353s 797ms/step - loss: 1.4752 - accuracy: 0.4070 - val_loss: 1.1930 - val_accuracy: 0.5284 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4642 - accuracy: 0.4193\n",
            "Epoch 00041: accuracy improved from 0.40887 to 0.41925, saving model to emotions.h5\n",
            "443/443 [==============================] - 339s 766ms/step - loss: 1.4642 - accuracy: 0.4193 - val_loss: 1.2871 - val_accuracy: 0.4903 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4506 - accuracy: 0.4205\n",
            "Epoch 00042: accuracy improved from 0.41925 to 0.42050, saving model to emotions.h5\n",
            "443/443 [==============================] - 341s 769ms/step - loss: 1.4506 - accuracy: 0.4205 - val_loss: 1.2283 - val_accuracy: 0.5125 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4589 - accuracy: 0.4193\n",
            "Epoch 00043: accuracy did not improve from 0.42050\n",
            "443/443 [==============================] - 342s 772ms/step - loss: 1.4589 - accuracy: 0.4193 - val_loss: 1.3058 - val_accuracy: 0.4835 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4576 - accuracy: 0.4195\n",
            "Epoch 00044: accuracy did not improve from 0.42050\n",
            "443/443 [==============================] - 341s 769ms/step - loss: 1.4576 - accuracy: 0.4195 - val_loss: 1.2422 - val_accuracy: 0.5136 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4487 - accuracy: 0.4225\n",
            "Epoch 00045: accuracy improved from 0.42050 to 0.42247, saving model to emotions.h5\n",
            "443/443 [==============================] - 339s 764ms/step - loss: 1.4487 - accuracy: 0.4225 - val_loss: 1.3157 - val_accuracy: 0.4852 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4418 - accuracy: 0.4238\n",
            "Epoch 00046: accuracy improved from 0.42247 to 0.42384, saving model to emotions.h5\n",
            "443/443 [==============================] - 343s 774ms/step - loss: 1.4418 - accuracy: 0.4238 - val_loss: 1.2273 - val_accuracy: 0.5239 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4445 - accuracy: 0.4247\n",
            "Epoch 00047: accuracy improved from 0.42384 to 0.42473, saving model to emotions.h5\n",
            "443/443 [==============================] - 356s 803ms/step - loss: 1.4445 - accuracy: 0.4247 - val_loss: 1.2443 - val_accuracy: 0.5199 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4453 - accuracy: 0.4200\n",
            "Epoch 00048: accuracy did not improve from 0.42473\n",
            "443/443 [==============================] - 363s 819ms/step - loss: 1.4453 - accuracy: 0.4200 - val_loss: 1.2073 - val_accuracy: 0.5148 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4532 - accuracy: 0.4135\n",
            "Epoch 00049: accuracy did not improve from 0.42473\n",
            "443/443 [==============================] - 350s 790ms/step - loss: 1.4532 - accuracy: 0.4135 - val_loss: 1.2631 - val_accuracy: 0.5165 - lr: 0.0010\n",
            "Epoch 50/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4456 - accuracy: 0.4212\n",
            "Epoch 00050: accuracy did not improve from 0.42473\n",
            "443/443 [==============================] - 358s 807ms/step - loss: 1.4456 - accuracy: 0.4212 - val_loss: 1.2337 - val_accuracy: 0.5153 - lr: 0.0010\n",
            "Epoch 51/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4336 - accuracy: 0.4280\n",
            "Epoch 00051: accuracy improved from 0.42473 to 0.42801, saving model to emotions.h5\n",
            "443/443 [==============================] - 346s 780ms/step - loss: 1.4336 - accuracy: 0.4280 - val_loss: 1.2138 - val_accuracy: 0.5347 - lr: 0.0010\n",
            "Epoch 52/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4342 - accuracy: 0.4277\n",
            "Epoch 00052: accuracy did not improve from 0.42801\n",
            "443/443 [==============================] - 348s 785ms/step - loss: 1.4342 - accuracy: 0.4277 - val_loss: 1.2776 - val_accuracy: 0.5142 - lr: 0.0010\n",
            "Epoch 53/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4352 - accuracy: 0.4296\n",
            "Epoch 00053: accuracy improved from 0.42801 to 0.42960, saving model to emotions.h5\n",
            "443/443 [==============================] - 335s 756ms/step - loss: 1.4352 - accuracy: 0.4296 - val_loss: 1.2103 - val_accuracy: 0.5375 - lr: 0.0010\n",
            "Epoch 54/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4246 - accuracy: 0.4335\n",
            "Epoch 00054: accuracy improved from 0.42960 to 0.43351, saving model to emotions.h5\n",
            "443/443 [==============================] - 377s 850ms/step - loss: 1.4246 - accuracy: 0.4335 - val_loss: 1.2321 - val_accuracy: 0.5136 - lr: 0.0010\n",
            "Epoch 55/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4226 - accuracy: 0.4355\n",
            "Epoch 00055: accuracy improved from 0.43351 to 0.43552, saving model to emotions.h5\n",
            "443/443 [==============================] - 330s 746ms/step - loss: 1.4226 - accuracy: 0.4355 - val_loss: 1.1904 - val_accuracy: 0.5403 - lr: 0.0010\n",
            "Epoch 56/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4181 - accuracy: 0.4309\n",
            "Epoch 00056: accuracy did not improve from 0.43552\n",
            "443/443 [==============================] - 340s 767ms/step - loss: 1.4181 - accuracy: 0.4309 - val_loss: 1.1921 - val_accuracy: 0.5341 - lr: 0.0010\n",
            "Epoch 57/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4131 - accuracy: 0.4386\n",
            "Epoch 00057: accuracy improved from 0.43552 to 0.43860, saving model to emotions.h5\n",
            "443/443 [==============================] - 348s 786ms/step - loss: 1.4131 - accuracy: 0.4386 - val_loss: 1.2075 - val_accuracy: 0.5301 - lr: 0.0010\n",
            "Epoch 58/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4251 - accuracy: 0.4346\n",
            "Epoch 00058: accuracy did not improve from 0.43860\n",
            "443/443 [==============================] - 360s 812ms/step - loss: 1.4251 - accuracy: 0.4346 - val_loss: 1.2238 - val_accuracy: 0.5227 - lr: 0.0010\n",
            "Epoch 59/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4166 - accuracy: 0.4357\n",
            "Epoch 00059: accuracy did not improve from 0.43860\n",
            "443/443 [==============================] - 355s 801ms/step - loss: 1.4166 - accuracy: 0.4357 - val_loss: 1.1414 - val_accuracy: 0.5506 - lr: 0.0010\n",
            "Epoch 60/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4118 - accuracy: 0.4358\n",
            "Epoch 00060: accuracy did not improve from 0.43860\n",
            "443/443 [==============================] - 358s 809ms/step - loss: 1.4118 - accuracy: 0.4358 - val_loss: 1.1430 - val_accuracy: 0.5568 - lr: 0.0010\n",
            "Epoch 61/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4127 - accuracy: 0.4392\n",
            "Epoch 00061: accuracy improved from 0.43860 to 0.43916, saving model to emotions.h5\n",
            "443/443 [==============================] - 343s 775ms/step - loss: 1.4127 - accuracy: 0.4392 - val_loss: 1.1782 - val_accuracy: 0.5278 - lr: 0.0010\n",
            "Epoch 62/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4044 - accuracy: 0.4429\n",
            "Epoch 00062: accuracy improved from 0.43916 to 0.44293, saving model to emotions.h5\n",
            "443/443 [==============================] - 376s 848ms/step - loss: 1.4044 - accuracy: 0.4429 - val_loss: 1.1693 - val_accuracy: 0.5523 - lr: 0.0010\n",
            "Epoch 63/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4080 - accuracy: 0.4471\n",
            "Epoch 00063: accuracy improved from 0.44293 to 0.44707, saving model to emotions.h5\n",
            "443/443 [==============================] - 394s 888ms/step - loss: 1.4080 - accuracy: 0.4471 - val_loss: 1.2778 - val_accuracy: 0.5097 - lr: 0.0010\n",
            "Epoch 64/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4120 - accuracy: 0.4406\n",
            "Epoch 00064: accuracy did not improve from 0.44707\n",
            "443/443 [==============================] - 342s 772ms/step - loss: 1.4120 - accuracy: 0.4406 - val_loss: 1.1727 - val_accuracy: 0.5460 - lr: 0.0010\n",
            "Epoch 65/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4091 - accuracy: 0.4414\n",
            "Epoch 00065: accuracy did not improve from 0.44707\n",
            "443/443 [==============================] - 344s 777ms/step - loss: 1.4091 - accuracy: 0.4414 - val_loss: 1.1333 - val_accuracy: 0.5597 - lr: 0.0010\n",
            "Epoch 66/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4150 - accuracy: 0.4391\n",
            "Epoch 00066: accuracy did not improve from 0.44707\n",
            "443/443 [==============================] - 334s 753ms/step - loss: 1.4150 - accuracy: 0.4391 - val_loss: 1.1217 - val_accuracy: 0.5727 - lr: 0.0010\n",
            "Epoch 67/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4080 - accuracy: 0.4415\n",
            "Epoch 00067: accuracy did not improve from 0.44707\n",
            "443/443 [==============================] - 334s 755ms/step - loss: 1.4080 - accuracy: 0.4415 - val_loss: 1.1184 - val_accuracy: 0.5568 - lr: 0.0010\n",
            "Epoch 68/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3959 - accuracy: 0.4452\n",
            "Epoch 00068: accuracy did not improve from 0.44707\n",
            "443/443 [==============================] - 347s 782ms/step - loss: 1.3959 - accuracy: 0.4452 - val_loss: 1.1496 - val_accuracy: 0.5568 - lr: 0.0010\n",
            "Epoch 69/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3893 - accuracy: 0.4526\n",
            "Epoch 00069: accuracy improved from 0.44707 to 0.45260, saving model to emotions.h5\n",
            "443/443 [==============================] - 341s 769ms/step - loss: 1.3893 - accuracy: 0.4526 - val_loss: 1.1401 - val_accuracy: 0.5364 - lr: 0.0010\n",
            "Epoch 70/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3976 - accuracy: 0.4403\n",
            "Epoch 00070: accuracy did not improve from 0.45260\n",
            "443/443 [==============================] - 337s 762ms/step - loss: 1.3976 - accuracy: 0.4403 - val_loss: 1.1230 - val_accuracy: 0.5653 - lr: 0.0010\n",
            "Epoch 71/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3955 - accuracy: 0.4527\n",
            "Epoch 00071: accuracy improved from 0.45260 to 0.45265, saving model to emotions.h5\n",
            "443/443 [==============================] - 346s 782ms/step - loss: 1.3955 - accuracy: 0.4527 - val_loss: 1.1679 - val_accuracy: 0.5534 - lr: 0.0010\n",
            "Epoch 72/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.4052 - accuracy: 0.4443\n",
            "Epoch 00072: accuracy did not improve from 0.45265\n",
            "443/443 [==============================] - 347s 784ms/step - loss: 1.4052 - accuracy: 0.4443 - val_loss: 1.1517 - val_accuracy: 0.5386 - lr: 0.0010\n",
            "Epoch 73/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3850 - accuracy: 0.4520\n",
            "Epoch 00073: accuracy did not improve from 0.45265\n",
            "443/443 [==============================] - 337s 761ms/step - loss: 1.3850 - accuracy: 0.4520 - val_loss: 1.1572 - val_accuracy: 0.5460 - lr: 0.0010\n",
            "Epoch 74/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3925 - accuracy: 0.4467\n",
            "Epoch 00074: accuracy did not improve from 0.45265\n",
            "443/443 [==============================] - 335s 757ms/step - loss: 1.3925 - accuracy: 0.4467 - val_loss: 1.1467 - val_accuracy: 0.5494 - lr: 0.0010\n",
            "Epoch 75/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3824 - accuracy: 0.4563\n",
            "Epoch 00075: accuracy improved from 0.45265 to 0.45626, saving model to emotions.h5\n",
            "443/443 [==============================] - 360s 814ms/step - loss: 1.3824 - accuracy: 0.4563 - val_loss: 1.1413 - val_accuracy: 0.5426 - lr: 0.0010\n",
            "Epoch 76/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3746 - accuracy: 0.4560\n",
            "Epoch 00076: accuracy did not improve from 0.45626\n",
            "443/443 [==============================] - 353s 796ms/step - loss: 1.3746 - accuracy: 0.4560 - val_loss: 1.0835 - val_accuracy: 0.5699 - lr: 0.0010\n",
            "Epoch 77/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3904 - accuracy: 0.4524\n",
            "Epoch 00077: accuracy did not improve from 0.45626\n",
            "443/443 [==============================] - 351s 793ms/step - loss: 1.3904 - accuracy: 0.4524 - val_loss: 1.1377 - val_accuracy: 0.5562 - lr: 0.0010\n",
            "Epoch 78/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3980 - accuracy: 0.4492\n",
            "Epoch 00078: accuracy did not improve from 0.45626\n",
            "443/443 [==============================] - 341s 769ms/step - loss: 1.3980 - accuracy: 0.4492 - val_loss: 1.1019 - val_accuracy: 0.5540 - lr: 0.0010\n",
            "Epoch 79/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3906 - accuracy: 0.4504\n",
            "Epoch 00079: accuracy did not improve from 0.45626\n",
            "443/443 [==============================] - 345s 778ms/step - loss: 1.3906 - accuracy: 0.4504 - val_loss: 1.1227 - val_accuracy: 0.5653 - lr: 0.0010\n",
            "Epoch 80/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3850 - accuracy: 0.4541\n",
            "Epoch 00080: accuracy did not improve from 0.45626\n",
            "443/443 [==============================] - 364s 822ms/step - loss: 1.3850 - accuracy: 0.4541 - val_loss: 1.2393 - val_accuracy: 0.5142 - lr: 0.0010\n",
            "Epoch 81/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3775 - accuracy: 0.4539\n",
            "Epoch 00081: accuracy did not improve from 0.45626\n",
            "443/443 [==============================] - 365s 823ms/step - loss: 1.3775 - accuracy: 0.4539 - val_loss: 1.1080 - val_accuracy: 0.5847 - lr: 0.0010\n",
            "Epoch 82/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3882 - accuracy: 0.4522\n",
            "Epoch 00082: accuracy did not improve from 0.45626\n",
            "443/443 [==============================] - 365s 823ms/step - loss: 1.3882 - accuracy: 0.4522 - val_loss: 1.1188 - val_accuracy: 0.5733 - lr: 0.0010\n",
            "Epoch 83/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3746 - accuracy: 0.4616\n",
            "Epoch 00083: accuracy improved from 0.45626 to 0.46163, saving model to emotions.h5\n",
            "443/443 [==============================] - 353s 797ms/step - loss: 1.3746 - accuracy: 0.4616 - val_loss: 1.1477 - val_accuracy: 0.5483 - lr: 0.0010\n",
            "Epoch 84/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3813 - accuracy: 0.4548\n",
            "Epoch 00084: accuracy did not improve from 0.46163\n",
            "443/443 [==============================] - 335s 756ms/step - loss: 1.3813 - accuracy: 0.4548 - val_loss: 1.1218 - val_accuracy: 0.5614 - lr: 0.0010\n",
            "Epoch 85/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3761 - accuracy: 0.4583\n",
            "Epoch 00085: accuracy did not improve from 0.46163\n",
            "443/443 [==============================] - 356s 804ms/step - loss: 1.3761 - accuracy: 0.4583 - val_loss: 1.1915 - val_accuracy: 0.5443 - lr: 0.0010\n",
            "Epoch 86/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3738 - accuracy: 0.4642\n",
            "Epoch 00086: accuracy improved from 0.46163 to 0.46416, saving model to emotions.h5\n",
            "443/443 [==============================] - 366s 826ms/step - loss: 1.3738 - accuracy: 0.4642 - val_loss: 1.0841 - val_accuracy: 0.5847 - lr: 0.0010\n",
            "Epoch 87/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3730 - accuracy: 0.4601\n",
            "Epoch 00087: accuracy did not improve from 0.46416\n",
            "443/443 [==============================] - 369s 834ms/step - loss: 1.3730 - accuracy: 0.4601 - val_loss: 1.1451 - val_accuracy: 0.5591 - lr: 0.0010\n",
            "Epoch 88/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3699 - accuracy: 0.4610\n",
            "Epoch 00088: accuracy did not improve from 0.46416\n",
            "443/443 [==============================] - 377s 850ms/step - loss: 1.3699 - accuracy: 0.4610 - val_loss: 1.1366 - val_accuracy: 0.5682 - lr: 0.0010\n",
            "Epoch 89/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3732 - accuracy: 0.4575\n",
            "Epoch 00089: accuracy did not improve from 0.46416\n",
            "443/443 [==============================] - 354s 798ms/step - loss: 1.3732 - accuracy: 0.4575 - val_loss: 1.1339 - val_accuracy: 0.5699 - lr: 0.0010\n",
            "Epoch 90/150\n",
            "443/443 [==============================] - ETA: 0s - loss: 1.3765 - accuracy: 0.4577\n",
            "Epoch 00090: accuracy did not improve from 0.46416\n",
            "443/443 [==============================] - 364s 821ms/step - loss: 1.3765 - accuracy: 0.4577 - val_loss: 1.0769 - val_accuracy: 0.5682 - lr: 0.0010\n",
            "Epoch 91/150\n",
            "266/443 [=================>............] - ETA: 2:15 - loss: 1.3740 - accuracy: 0.4569"
          ]
        }
      ]
    }
  ]
}